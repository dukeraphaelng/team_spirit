{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2V0zUN2adLl",
    "outputId": "e0d6007b-7cca-494d-a122-f0dfbbd35f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NHYa0fZ7ajgl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Classifier\n",
    "from sklearn.svm import SVC\n",
    "# Character N-gram feature extractor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Util\n",
    "from data_io import get_book\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Keras \n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNb80DPAauyM"
   },
   "source": [
    "**Create the training, test and validation sets**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMzy7Auiaq5L"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "val_data = pd.read_csv(\"val.csv\")\n",
    "\n",
    "cv = CountVectorizer(analyzer='char', ngram_range=(1, 5), dtype=np.float32, max_features=10000)\n",
    "X_train, X_test, X_val = cv.fit_transform(train_data.text.tolist()), cv.transform(test_data.text.tolist()), cv.transform(val_data.text.tolist())  \n",
    "Y_train, Y_test, Y_val = train_data.author.tolist(), test_data.author.tolist(), val_data.author.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_47vJnX8axRC"
   },
   "source": [
    "**Turn labelled data into numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLf_7b2Jawm3"
   },
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Y_train = Encoder.fit_transform(Y_train)\n",
    "Y_test = Encoder.transform(Y_test)\n",
    "Y_val = Encoder.transform(Y_val)\n",
    "\n",
    "print(np.array(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxWiB2Mfa44I"
   },
   "source": [
    "**Just seeing the distribution of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6WHsJcEa7ye"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(cv.vocabulary_)\n",
    "print(len(train_data.text.tolist()[1].split(\" \")))\n",
    "\n",
    "one_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 1]\n",
    "print(\"total occurrences of length 1 char n-gram\")\n",
    "print(sum(one_ngram_occurrences))\n",
    "print(\"total amount of length 1 char n-gram\")\n",
    "print(len(one_ngram_occurrences))\n",
    "\n",
    "two_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 2]\n",
    "print(\"total occurrences of length 2 char n-gram\")\n",
    "print(sum(two_ngram_occurrences))\n",
    "print(\"total amount of length 2 char n-gram\")\n",
    "print(len(two_ngram_occurrences))\n",
    "\n",
    "three_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 3]\n",
    "print(\"total occurrences of length 3 char n-gram\")\n",
    "print(sum(three_ngram_occurrences))\n",
    "print(\"total amount of length 3 char n-gram\")\n",
    "print(len(three_ngram_occurrences))\n",
    "\n",
    "four_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 4]\n",
    "print(\"total occurrences of length 4 char n-gram\")\n",
    "print(sum(four_ngram_occurrences))\n",
    "print(\"total amount of length 4 char n-gram\")\n",
    "print(len(four_ngram_occurrences))\n",
    "\n",
    "five_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 5]\n",
    "print(\"total occurrences of length 5 char n-gram\")\n",
    "print(sum(five_ngram_occurrences))\n",
    "print(\"total amount of length 5 char n-gram\")\n",
    "print(len(five_ngram_occurrences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0JCcruLa-pf"
   },
   "source": [
    "**Scale the data using min max normalizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSuP6NGnbBFH"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_test, X_val = X_train.toarray(), X_test.toarray(), X_val.toarray()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0tZkm7ebFoz"
   },
   "source": [
    "**First, just apply SVM on the raw input without encoding with autoencoders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-ao1sPybDNk"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X_train_scaled, Y_train)\n",
    "\n",
    "preds = svm.predict(X_test_scaled)\n",
    "\n",
    "print(Y_test)\n",
    "print(\"############################################\")\n",
    "print(preds)\n",
    "print(\"SVM Accuracy Score on test -> \", accuracy_score(preds, Y_test)*100)\n",
    "\n",
    "print(\"SVM Accuracy Score on training -> \", accuracy_score(svm.predict(X_train_scaled), Y_train)*100)\n",
    "\n",
    "print(\"SVM Accuracy Score on validation -> \", accuracy_score(svm.predict(X_val_scaled), Y_val)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etDUqknUbUV4"
   },
   "source": [
    "**Code for the stacked denoising autoencoders**\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-JrxPuFbYpO"
   },
   "source": [
    "**Class for tying weights in a denoising auto encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd5qb5cpbdhj"
   },
   "outputs": [],
   "source": [
    "# Code referenced from https://medium.com/@lmayrandprovencher/building-an-autoencoder-with-tied-weights-in-keras-c4a559c529a2\n",
    "\n",
    "class DenseTranspose(keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\",\n",
    "                                      shape=[self.dense.input_shape[-1]],\n",
    "                                      initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwOQSUXsbils"
   },
   "source": [
    "**Class for construction of a denoising autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AZWA6N9bmWk"
   },
   "outputs": [],
   "source": [
    "np.random.seed(55)\n",
    "class DenoisingAutoEncoder:\n",
    "    def __init__(self, layers, corruption, activate_encoder, activate_decoder):\n",
    "        self.layers = layers\n",
    "        self.corruption = corruption\n",
    "        self.activate_encoder = activate_encoder\n",
    "        self.activate_decoder = activate_decoder\n",
    "\n",
    "  \n",
    "    def forward(self, X_train, X_val, epochs, batch_size):\n",
    "        # Step 1, Add binomial noise\n",
    "        X_train_noisy = self.inject_noise(X_train)\n",
    "\n",
    "        # Step 2, Encode X_train_noisy using sigmoid\n",
    "        encoder_input = Input(shape = (X_train_noisy.shape[1], ))\n",
    "        encoder = Dense(self.layers[0], activation=self.activate_encoder)\n",
    "        final_encoder = encoder(encoder_input)\n",
    "\n",
    "        # Step 3, Decode X_train_noisy using sigmoid\n",
    "        # Tie the weights between the encoder and decoder layers\n",
    "        decoder = DenseTranspose(encoder, activation=self.activate_decoder)\n",
    "        final_decoder = decoder(final_encoder)\n",
    "\n",
    "        # Step 4, cross entropy loss for normalised data and adam optimizer (Not sure what optimizer the paper uses)\n",
    "        autoencoder = Model(encoder_input, final_decoder)\n",
    "        autoencoder.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "\n",
    "        # Train it\n",
    "        autoencoder.fit(X_train_noisy, X_train, batch_size = batch_size, epochs = epochs, validation_data=(X_val, X_val))\n",
    "        autoencoder.summary()\n",
    "\n",
    "        # Get the model that maps input to its encoded representation\n",
    "        encoder_model = Model(encoder_input, final_encoder)\n",
    "\n",
    "        # Return the (encoding model, encoding function)\n",
    "        return (encoder_model, encoder)\n",
    "\n",
    "    def inject_noise(self, x):\n",
    "        # inject binomial noise since this model assumes you are normalising input \n",
    "        # with min max normalisation\n",
    "        mask = np.random.choice([0, 1], size=x.shape, p=[self.corruption, 1-self.corruption])\n",
    "        X_noisy = x * mask\n",
    "        return X_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0YQwys_b2oH"
   },
   "source": [
    "**Class for construction of a stacked denoising autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DxyEtio8b7g7"
   },
   "outputs": [],
   "source": [
    "class StackedDenoisingAutoEncoder():\n",
    "    def __init__(self, layers, corruption, activate_encoder, activate_decoder):\n",
    "        self.layers = layers\n",
    "        self.corruption = corruption\n",
    "        self.activate_encoder = activate_encoder\n",
    "        self.activate_decoder = activate_decoder\n",
    "        self.encoding_func = None\n",
    "        self.encoder_layer = None\n",
    "  \n",
    "    def pretrain(self, X_train, X_val, epochs, batch_size):\n",
    "        # self.layers contains the units each denoising autoencoder should take in\n",
    "        # After testing, maybe I implemented this wrong but when I used more than 1\n",
    "        # auto encoder, the performance dropped so now I have coded this so that\n",
    "        # it assumes that self.layers only contains 1 value\n",
    "\n",
    "        # If self.layers contains more than 1, then it will break since the\n",
    "        # self.encoding_func and self.encoder_layer will not represent the multiple\n",
    "        # denoising autoencoders\n",
    "        # Why did I keep it a list? I dont know\n",
    "        for layer in self.layers:\n",
    "            autoencoder = DenoisingAutoEncoder([layer], self.corruption, self.activate_encoder, self.activate_decoder)\n",
    "            (encoding_function, encoder) = autoencoder.forward(learnt_input, encoded_validation, epochs, batch_size)\n",
    "            learnt_input = encoding_function.predict(learnt_input)\n",
    "            encoded_validation = encoding_function.predict(encoded_validation)\n",
    "\n",
    "            self.encoding_func = encoding_function\n",
    "            self.encoder_layer = encoder\n",
    "\n",
    "    def finetune(self, X_train, Y_train, epochs, batch_size):\n",
    "        encoder_input = Input(shape = (X_train.shape[1], ))\n",
    "\n",
    "        encoder = self.encoder_layer\n",
    "        final_encoder = encoder(encoder_input)\n",
    "        # Define the logistic regression layer\n",
    "        lr_layer = Dense(Y_train.shape[1], activation='softmax')\n",
    "        predictions = lr_layer(final_encoder)\n",
    "\n",
    "        # Create the fine-tuned model\n",
    "        fine_tuned_model = Model(inputs=encoder_input, outputs=predictions)\n",
    "        fine_tuned_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "        fine_tuned_model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "        fine_tuned_encoder = Model(inputs=encoder_input, outputs=final_encoder)\n",
    "        return (fine_tuned_model, fine_tuned_encoder)\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.encoding_func.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEd-jwpqgoHZ"
   },
   "source": [
    "**Pretraining and Finetuning**\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1w5tdUiddbT"
   },
   "source": [
    "**Pretrain the denoising autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgJDtaVYdb71"
   },
   "outputs": [],
   "source": [
    "# The \"stacked\" auto encoder will only contain 1 denoising auto encoder that will \n",
    "# transform the original input into 1000 units. Noise corruption is 0.3 and it uses \n",
    "# sigmoid activation for both encoder and decoder\n",
    "stacked_auto_encoder = StackedDenoisingAutoEncoder([1000], 0.3, 'sigmoid', 'sigmoid')\n",
    "stacked_auto_encoder.pretrain(X_train_scaled, X_val_scaled, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u0Oipi3d4Vl"
   },
   "source": [
    "**Finetune the model by adding logistic regression layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et_S-qdugNC_"
   },
   "outputs": [],
   "source": [
    "# Create a 1 hot encoded Y_train\n",
    "Y_train_hot_encoded = []\n",
    "for author in Y_train:\n",
    "    Y_max = max(Y_train)\n",
    "    one_hot_encoded = [0] * (Y_max+1)\n",
    "    one_hot_encoded[author] = 1\n",
    "    Y_train_hot_encoded.append(one_hot_encoded)\n",
    "Y_train_hot_encoded = np.array(Y_train_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJlNeXIId30_"
   },
   "outputs": [],
   "source": [
    "fine_tuned_model, fine_tuned_encoder = stacked_auto_encoder.finetune(X_train_scaled, Y_train_hot_encoded, 100, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmWX_fDvgF_R"
   },
   "source": [
    "**Now feed the encoded representation into linear SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GWVbI8nc9H6"
   },
   "outputs": [],
   "source": [
    "X_train_encoded = fine_tuned_encoder.predict(X_train_scaled)\n",
    "\n",
    "# Fit to encoded data\n",
    "svm_autoencoder = SVC(kernel='linear', C=1)\n",
    "svm_autoencoder.fit(X_train_encoded, Y_train)\n",
    "\n",
    "# Encode the test data and use SVM to predict its labels\n",
    "X_test_encoded = fine_tuned_encoder.predict(X_test_scaled)\n",
    "predicted = svm_autoencoder.predict(X_test_encoded)\n",
    "\n",
    "print(Y_test)\n",
    "print(\"########################\")\n",
    "print(predicted)\n",
    "\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predicted, Y_test)*100)\n",
    "\n",
    "predicted_train = svm_autoencoder.predict(X_train_encoded)\n",
    "print(\"SVM Accuracy Score on training -> \", accuracy_score(predicted_train, Y_train)*100)\n",
    "\n",
    "X_val_encoded = fine_tuned_encoder.predict(X_val_scaled)\n",
    "predicted_val = svm_autoencoder.predict(X_val_encoded)\n",
    "print(\"SVM Accuracy Score on validation -> \", accuracy_score(predicted_val, Y_val)*100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
