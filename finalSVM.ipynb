{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2V0zUN2adLl",
        "outputId": "e0d6007b-7cca-494d-a122-f0dfbbd35f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Classifier\n",
        "from sklearn.svm import SVC\n",
        "# Character N-gram feature extractor\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Util\n",
        "from data_io import get_book\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Keras \n",
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "NHYa0fZ7ajgl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the training, test and validation sets**\n"
      ],
      "metadata": {
        "id": "lNb80DPAauyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "val_data = pd.read_csv(\"val.csv\")\n",
        "\n",
        "cv = CountVectorizer(analyzer='char', ngram_range=(1, 5), dtype=np.float32, max_features=10000)\n",
        "X_train, X_test, X_val = cv.fit_transform(train_data.text.tolist()), cv.transform(test_data.text.tolist()), cv.transform(val_data.text.tolist())  \n",
        "Y_train, Y_test, Y_val = train_data.author.tolist(), test_data.author.tolist(), val_data.author.tolist()"
      ],
      "metadata": {
        "id": "wMzy7Auiaq5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Turn labelled data into numbers**"
      ],
      "metadata": {
        "id": "_47vJnX8axRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder = LabelEncoder()\n",
        "Y_train = Encoder.fit_transform(Y_train)\n",
        "Y_test = Encoder.transform(Y_test)\n",
        "Y_val = Encoder.transform(Y_val)\n",
        "\n",
        "print(np.array(Y_train))"
      ],
      "metadata": {
        "id": "PLf_7b2Jawm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Just seeing the distribution of data**"
      ],
      "metadata": {
        "id": "VxWiB2Mfa44I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(cv.vocabulary_)\n",
        "print(len(train_data.text.tolist()[1].split(\" \")))\n",
        "\n",
        "one_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 1]\n",
        "print(\"total occurrences of length 1 char n-gram\")\n",
        "print(sum(one_ngram_occurrences))\n",
        "print(\"total amount of length 1 char n-gram\")\n",
        "print(len(one_ngram_occurrences))\n",
        "\n",
        "two_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 2]\n",
        "print(\"total occurrences of length 2 char n-gram\")\n",
        "print(sum(two_ngram_occurrences))\n",
        "print(\"total amount of length 2 char n-gram\")\n",
        "print(len(two_ngram_occurrences))\n",
        "\n",
        "three_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 3]\n",
        "print(\"total occurrences of length 3 char n-gram\")\n",
        "print(sum(three_ngram_occurrences))\n",
        "print(\"total amount of length 3 char n-gram\")\n",
        "print(len(three_ngram_occurrences))\n",
        "\n",
        "four_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 4]\n",
        "print(\"total occurrences of length 4 char n-gram\")\n",
        "print(sum(four_ngram_occurrences))\n",
        "print(\"total amount of length 4 char n-gram\")\n",
        "print(len(four_ngram_occurrences))\n",
        "\n",
        "five_ngram_occurrences = [cv.vocabulary_[ngram] for ngram in cv.vocabulary_ if len(ngram) == 5]\n",
        "print(\"total occurrences of length 5 char n-gram\")\n",
        "print(sum(five_ngram_occurrences))\n",
        "print(\"total amount of length 5 char n-gram\")\n",
        "print(len(five_ngram_occurrences))"
      ],
      "metadata": {
        "id": "L6WHsJcEa7ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scale the data using min max normalizer**"
      ],
      "metadata": {
        "id": "f0JCcruLa-pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X_train, X_test, X_val = X_train.toarray(), X_test.toarray(), X_val.toarray()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "print(X_train_scaled)"
      ],
      "metadata": {
        "id": "SSuP6NGnbBFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First, just apply SVM on the raw input without encoding with autoencoders**"
      ],
      "metadata": {
        "id": "b0tZkm7ebFoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(kernel='linear', C=1)\n",
        "svm.fit(X_train_scaled, Y_train)\n",
        "\n",
        "preds = svm.predict(X_test_scaled)\n",
        "\n",
        "print(Y_test)\n",
        "print(\"############################################\")\n",
        "print(preds)\n",
        "print(\"SVM Accuracy Score on test -> \", accuracy_score(preds, Y_test)*100)\n",
        "\n",
        "print(\"SVM Accuracy Score on training -> \", accuracy_score(svm.predict(X_train_scaled), Y_train)*100)\n",
        "\n",
        "print(\"SVM Accuracy Score on validation -> \", accuracy_score(svm.predict(X_val_scaled), Y_val)*100)"
      ],
      "metadata": {
        "id": "M-ao1sPybDNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for the stacked denoising autoencoders**\n",
        "=="
      ],
      "metadata": {
        "id": "etDUqknUbUV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class for tying weights in a denoising auto encoder**"
      ],
      "metadata": {
        "id": "m-JrxPuFbYpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code referenced from https://medium.com/@lmayrandprovencher/building-an-autoencoder-with-tied-weights-in-keras-c4a559c529a2\n",
        "\n",
        "class DenseTranspose(keras.layers.Layer):\n",
        "  def __init__(self, dense, activation=None, **kwargs):\n",
        "    self.dense = dense\n",
        "    self.activation = keras.activations.get(activation)\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def build(self, batch_input_shape):\n",
        "    self.biases = self.add_weight(name=\"bias\",\n",
        "                                  shape=[self.dense.input_shape[-1]],\n",
        "                                  initializer=\"zeros\")\n",
        "    super().build(batch_input_shape)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
        "    return self.activation(z + self.biases)"
      ],
      "metadata": {
        "id": "cd5qb5cpbdhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class for construction of a denoising autoencoder**"
      ],
      "metadata": {
        "id": "qwOQSUXsbils"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(55)\n",
        "class DenoisingAutoEncoder:\n",
        "  def __init__(self, layers, corruption, activate_encoder, activate_decoder):\n",
        "    self.layers = layers\n",
        "    self.corruption = corruption\n",
        "    self.activate_encoder = activate_encoder\n",
        "    self.activate_decoder = activate_decoder\n",
        "\n",
        "  \n",
        "  def forward(self, X_train, X_val, epochs, batch_size):\n",
        "    # Step 1, Add binomial noise\n",
        "    X_train_noisy = self.inject_noise(X_train)\n",
        "\n",
        "    # Step 2, Encode X_train_noisy using sigmoid\n",
        "    encoder_input = Input(shape = (X_train_noisy.shape[1], ))\n",
        "    encoder = Dense(self.layers[0], activation=self.activate_encoder)\n",
        "    final_encoder = encoder(encoder_input)\n",
        "\n",
        "    # Step 3, Decode X_train_noisy using sigmoid\n",
        "    # Tie the weights between the encoder and decoder layers\n",
        "    decoder = DenseTranspose(encoder, activation=self.activate_decoder)\n",
        "    final_decoder = decoder(final_encoder)\n",
        "\n",
        "    # Step 4, cross entropy loss for normalised data and adam optimizer (Not sure what optimizer the paper uses)\n",
        "    autoencoder = Model(encoder_input, final_decoder)\n",
        "    autoencoder.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
        "\n",
        "    # Train it\n",
        "    autoencoder.fit(X_train_noisy, X_train, batch_size = batch_size, epochs = epochs, validation_data=(X_val, X_val))\n",
        "    autoencoder.summary()\n",
        "\n",
        "    # Get the model that maps input to its encoded representation\n",
        "    encoder_model = Model(encoder_input, final_encoder)\n",
        "\n",
        "    # Return the (encoding model, encoding function)\n",
        "    return (encoder_model, encoder)\n",
        "\n",
        "  def inject_noise(self, x):\n",
        "    # inject binomial noise since this model assumes you are normalising input \n",
        "    # with min max normalisation\n",
        "    mask = np.random.choice([0, 1], size=x.shape, p=[self.corruption, 1-self.corruption])\n",
        "    X_noisy = x * mask\n",
        "    return X_noisy"
      ],
      "metadata": {
        "id": "0AZWA6N9bmWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class for construction of a stacked denoising autoencoder**"
      ],
      "metadata": {
        "id": "Q0YQwys_b2oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StackedDenoisingAutoEncoder():\n",
        "  def __init__(self, layers, corruption, activate_encoder, activate_decoder):\n",
        "    self.layers = layers\n",
        "    self.corruption = corruption\n",
        "    self.activate_encoder = activate_encoder\n",
        "    self.activate_decoder = activate_decoder\n",
        "    self.encoding_func = None\n",
        "    self.encoder_layer = None\n",
        "  \n",
        "  def pretrain(self, X_train, X_val, epochs, batch_size):\n",
        "    # self.layers contains the units each denoising autoencoder should take in\n",
        "    # After testing, maybe I implemented this wrong but when I used more than 1\n",
        "    # auto encoder, the performance dropped so now I have coded this so that\n",
        "    # it assumes that self.layers only contains 1 value\n",
        "\n",
        "    # If self.layers contains more than 1, then it will break since the\n",
        "    # self.encoding_func and self.encoder_layer will not represent the multiple\n",
        "    # denoising autoencoders\n",
        "    # Why did I keep it a list? I dont know\n",
        "    for layer in self.layers:\n",
        "      autoencoder = DenoisingAutoEncoder([layer], self.corruption, self.activate_encoder, self.activate_decoder)\n",
        "      (encoding_function, encoder) = autoencoder.forward(learnt_input, encoded_validation, epochs, batch_size)\n",
        "      learnt_input = encoding_function.predict(learnt_input)\n",
        "      encoded_validation = encoding_function.predict(encoded_validation)\n",
        "\n",
        "      self.encoding_func = encoding_function\n",
        "      self.encoder_layer = encoder\n",
        "\n",
        "  def finetune(self, X_train, Y_train, epochs, batch_size):\n",
        "    encoder_input = Input(shape = (X_train.shape[1], ))\n",
        "\n",
        "    encoder = self.encoder_layer\n",
        "    final_encoder = encoder(encoder_input)\n",
        "    # Define the logistic regression layer\n",
        "    lr_layer = Dense(Y_train.shape[1], activation='softmax')\n",
        "    predictions = lr_layer(final_encoder)\n",
        "\n",
        "    # Create the fine-tuned model\n",
        "    fine_tuned_model = Model(inputs=encoder_input, outputs=predictions)\n",
        "    fine_tuned_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    fine_tuned_model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
        "    fine_tuned_encoder = Model(inputs=encoder_input, outputs=final_encoder)\n",
        "    return (fine_tuned_model, fine_tuned_encoder)\n",
        "\n",
        "  def encode(self, X):\n",
        "    return self.encoding_func.predict(X)"
      ],
      "metadata": {
        "id": "DxyEtio8b7g7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretraining and Finetuning**\n",
        "=="
      ],
      "metadata": {
        "id": "rEd-jwpqgoHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrain the denoising autoencoder**"
      ],
      "metadata": {
        "id": "H1w5tdUiddbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The \"stacked\" auto encoder will only contain 1 denoising auto encoder that will \n",
        "# transform the original input into 1000 units. Noise corruption is 0.3 and it uses \n",
        "# sigmoid activation for both encoder and decoder\n",
        "stacked_auto_encoder = StackedDenoisingAutoEncoder([1000], 0.3, 'sigmoid', 'sigmoid')\n",
        "stacked_auto_encoder.pretrain(X_train_scaled, X_val_scaled, 50, 1)"
      ],
      "metadata": {
        "id": "UgJDtaVYdb71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finetune the model by adding logistic regression layer**"
      ],
      "metadata": {
        "id": "9u0Oipi3d4Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 1 hot encoded Y_train\n",
        "Y_train_hot_encoded = []\n",
        "for author in Y_train:\n",
        "  Y_max = max(Y_train)\n",
        "  one_hot_encoded = [0] * (Y_max+1)\n",
        "  one_hot_encoded[author] = 1\n",
        "  Y_train_hot_encoded.append(one_hot_encoded)\n",
        "Y_train_hot_encoded = np.array(Y_train_hot_encoded)"
      ],
      "metadata": {
        "id": "Et_S-qdugNC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model, fine_tuned_encoder = stacked_auto_encoder.finetune(X_train_scaled, Y_train_hot_encoded, 100, 1) "
      ],
      "metadata": {
        "id": "zJlNeXIId30_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now feed the encoded representation into linear SVM**"
      ],
      "metadata": {
        "id": "vmWX_fDvgF_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoded = fine_tuned_encoder.predict(X_train_scaled)\n",
        "\n",
        "# Fit to encoded data\n",
        "svm_autoencoder = SVC(kernel='linear', C=1)\n",
        "svm_autoencoder.fit(X_train_encoded, Y_train)\n",
        "\n",
        "# Encode the test data and use SVM to predict its labels\n",
        "X_test_encoded = fine_tuned_encoder.predict(X_test_scaled)\n",
        "predicted = svm_autoencoder.predict(X_test_encoded)\n",
        "\n",
        "print(Y_test)\n",
        "print(\"########################\")\n",
        "print(predicted)\n",
        "\n",
        "print(\"SVM Accuracy Score -> \", accuracy_score(predicted, Y_test)*100)\n",
        "\n",
        "predicted_train = svm_autoencoder.predict(X_train_encoded)\n",
        "print(\"SVM Accuracy Score on training -> \", accuracy_score(predicted_train, Y_train)*100)\n",
        "\n",
        "X_val_encoded = fine_tuned_encoder.predict(X_val_scaled)\n",
        "predicted_val = svm_autoencoder.predict(X_val_encoded)\n",
        "print(\"SVM Accuracy Score on validation -> \", accuracy_score(predicted_val, Y_val)*100)"
      ],
      "metadata": {
        "id": "3GWVbI8nc9H6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}