{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-06T21:19:54.659245Z",
     "end_time": "2023-04-06T21:19:57.654613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CATALYST_LOG_LEVEL=15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "\n",
    "# Hide logging messages.\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "%env CATALYST_LOG_LEVEL = 15\n",
    "#!pip install tensorflow-macos\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "#set(val_df['author'].tolist()+train_df['author'].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T21:19:59.026080Z",
     "end_time": "2023-04-06T21:19:59.041828Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "bert_path = 'bert-base-uncased'\n",
    "\n",
    "\n",
    "def encode(input_text, max_len):\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        input_text,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation=True)\n",
    "\n",
    "    return inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T20:10:15.717090Z",
     "end_time": "2023-04-06T20:10:15.728440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "#train_df.drop(['index', 'id'], axis=1, inplace=True)\n",
    "max_len = max(len(x.split()) for x in train_df['text'])\n",
    "#max_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T20:10:16.665166Z",
     "end_time": "2023-04-06T20:10:16.676469Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_model(max_len, num_outputs=47, embeddings=\"\"):\n",
    "    input_word_ids = tf.keras.Input(\n",
    "        shape=(max_len,),\n",
    "        dtype=tf.int32,\n",
    "        name=\"input_word_ids\",\n",
    "    )\n",
    "\n",
    "    bert_encoder = TFBertModel.from_pretrained(bert_path, output_hidden_states=True)\n",
    "    bert_output = bert_encoder(input_word_ids)\n",
    "\n",
    "\n",
    "    x = None\n",
    "\n",
    "    if embeddings == \"last_4\":\n",
    "        # Concat hidden states from the last 4 layers instead of just the last 1.\n",
    "        bert_embeddings = tf.concat(bert_output[2][-4:], -1)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(bert_embeddings)\n",
    "    elif embeddings == \"pooler\":\n",
    "        x = bert_output[1]\n",
    "    else:\n",
    "        # Hidden states from the last layer.\n",
    "        bert_embeddings = bert_output[0]\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(bert_embeddings)\n",
    "\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    output = tf.keras.layers.Dense(num_outputs, )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=[\n",
    "                      tf.keras.losses.BinaryCrossentropy(\n",
    "                          from_logits=True, name='binary_crossentropy'),\n",
    "                      'accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T20:17:05.383378Z",
     "end_time": "2023-04-06T20:17:05.388447Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_word_ids (InputLayer)  [(None, 512)]            0         \n",
      "                                                                 \n",
      " tf_bert_model_4 (TFBertMode  TFBaseModelOutputWithPoo  109482240\n",
      " l)                          lingAndCrossAttentions(l            \n",
      "                             ast_hidden_state=(None,             \n",
      "                             512, 768),                          \n",
      "                              pooler_output=(None, 76            \n",
      "                             8),                                 \n",
      "                              past_key_values=None, h            \n",
      "                             idden_states=((None, 512            \n",
      "                             , 768),                             \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768),                  \n",
      "                              (None, 512, 768)),                 \n",
      "                              attentions=None, cross_            \n",
      "                             attentions=None)                    \n",
      "                                                                 \n",
      " global_average_pooling1d_4   (None, 768)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " layer_normalization_4 (Laye  (None, 768)              1536      \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 47)                36143     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,519,919\n",
      "Trainable params: 109,519,919\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#train_input = encode(train_df['text'].values.tolist(), max_len)[\"input_ids\"]\n",
    "train_sample_x = train_df['text'].tolist()\n",
    "train_y = train_df.drop('text',axis=1)\n",
    "val_sample_x = val_df['text'].tolist()\n",
    "inputs=encode(train_sample_x, 512)\n",
    "train_x = inputs[\"input_ids\"]\n",
    "val_x = encode(val_sample_x, 512)[\"input_ids\"]\n",
    "val_y=val_df.drop('text',axis=1)\n",
    "mask= inputs['attention_mask']\n",
    "train_ds = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((train_x, train_y))\n",
    "    .batch(batch_size=2)\n",
    "    .repeat()\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "val_ds = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((val_x, val_y))\n",
    "    .batch(2)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_binary_crossentropy',\n",
    "    patience=5,  # No early stopping\n",
    "    restore_best_weights=True,  # Despite no early stopping, use the best weights\n",
    ")\n",
    "model = create_model(512)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T20:17:06.339085Z",
     "end_time": "2023-04-06T20:17:09.560004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 25s 25s/step - loss: 0.9811 - binary_crossentropy: 0.9811 - accuracy: 0.0000e+00 - val_loss: 0.8672 - val_binary_crossentropy: 0.8675 - val_accuracy: 0.0213\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.9008 - binary_crossentropy: 0.9008 - accuracy: 0.0000e+00 - val_loss: 0.7505 - val_binary_crossentropy: 0.7509 - val_accuracy: 0.0213\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.7645 - binary_crossentropy: 0.7645 - accuracy: 0.0000e+00 - val_loss: 0.6422 - val_binary_crossentropy: 0.6426 - val_accuracy: 0.0213\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.6210 - binary_crossentropy: 0.6210 - accuracy: 0.0000e+00 - val_loss: 0.5451 - val_binary_crossentropy: 0.5456 - val_accuracy: 0.0213\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.5272 - binary_crossentropy: 0.5272 - accuracy: 0.0000e+00 - val_loss: 0.4614 - val_binary_crossentropy: 0.4617 - val_accuracy: 0.0213\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=1,\n",
    "    batch_size=2,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=val_ds,\n",
    "    validation_batch_size=2,\n",
    ")\n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "for tl in history.history[\"loss\"]:\n",
    "    train_loss_epochs.append(tl)\n",
    "for vl in history.history[\"val_loss\"]:\n",
    "    val_loss_epochs.append(vl)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T20:17:11.109186Z",
     "end_time": "2023-04-06T20:18:43.544714Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 47])\n",
      "Epoch 1: Dev accuracy = 0.02308736927807331\n",
      "torch.Size([47, 47])\n",
      "Epoch 2: Dev accuracy = 0.020823901519179344\n",
      "torch.Size([47, 47])\n",
      "Epoch 3: Dev accuracy = 0.023540062829852104\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Freeze the weights of the first layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.pooler = torch.nn.AdaptiveMaxPool1d(47)\n",
    "\n",
    "# Replace the classification layer with a linear layer\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(model.config.hidden_size, 47)\n",
    ")\n",
    "\n",
    "\n",
    "# Unfreeze the weights of the last 4 layers\n",
    "for param in model.encoder.layer[-4:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Fine-tune the model on your downstream task\n",
    "train_dataset = train_df\n",
    "dev_dataset = val_df\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "for epoch in range(3):\n",
    "    inputs = tokenizer(train_dataset['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "    labels = torch.tensor(train_dataset.drop('text',axis=1).values)\n",
    "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'])\n",
    "    pooled_output = outputs.pooler_output\n",
    "    #print(pooled_output.shape)\n",
    "    #print(labels.shape)\n",
    "    logits = model.pooler(pooled_output)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # Evaluate the model on the dev set\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(dev_dataset['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "        labels = torch.tensor(dev_dataset.drop('text',axis=1).values)\n",
    "        outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'])\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = model.pooler(pooled_output)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == labels).float().mean().item()\n",
    "    print(f\"Epoch {epoch+1}: Dev accuracy = {acc}\")\n",
    "\n",
    "# Use the fine-tuned model to make predictions on new data\n",
    "test_dataset = test_df\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_dataset['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'])\n",
    "    pooled_output = outputs.pooler_output\n",
    "    logits = model.pooler(pooled_output)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    test_dataset['pred'] = preds.tolist()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T21:23:23.857275Z",
     "end_time": "2023-04-06T21:25:12.312714Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
