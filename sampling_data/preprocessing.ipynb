{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from data_io import get_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filename = 'metadata.csv'\n",
    "counts_dirname = 'counts'\n",
    "tokens_dirname = 'tokens'\n",
    "\n",
    "metadata_df = pd.read_csv(metadata_filename)\n",
    "\n",
    "filtered_df = metadata_df[(metadata_df.language == \"['en']\") & (metadata_df.type == 'Text')]\n",
    "\n",
    "SELECTED_COLUMNS = ['id', 'title', 'author', 'authoryearofbirth', 'authoryearofdeath']\n",
    "filtered_df = filtered_df.dropna(subset=SELECTED_COLUMNS)\n",
    "filtered_df = filtered_df[SELECTED_COLUMNS]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "author_count = filtered_df['author'].value_counts()\n",
    "many_works_author = author_count[author_count >= 50]\n",
    "filtered_df = filtered_df[filtered_df.author.isin(many_works_author.index.to_numpy())].reset_index()\n",
    "\n",
    "'PG8700' in filtered_df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df = filtered_df.sample(n=50, random_state=2).reset_index()\n",
    "\n",
    "sampled_authors = filtered_df.author.sample(n=50, random_state=1)\n",
    "# Some author names are duplicated, the set contains 30 authors\n",
    "print(len(list(sampled_authors)))\n",
    "print(len(set(sampled_authors)))\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "val_ids = []\n",
    "\n",
    "# Split train, test, validation, total 50 books per author, total 30 authors\n",
    "# 60% = 30 books into training set\n",
    "# 30% = 15 books into test set\n",
    "# 10% = 5 books into validation set\n",
    "print(len(set(sampled_authors)))\n",
    "for author in set(sampled_authors):\n",
    "    works = filtered_df[filtered_df.author == author].sample(n=50, random_state=1)\n",
    "    works_list = list(works.id)\n",
    "    train_id, test_id, val_id = works_list[:30], works_list[30:45], works_list[45:]\n",
    "    \n",
    "    # Does not check if this file exists and is valid\n",
    "    train_ids.extend(train_id)\n",
    "    test_ids.extend(test_id)\n",
    "    val_ids.extend(val_id)\n",
    "\n",
    "train_df = filtered_df[filtered_df.id.isin(train_ids)]\n",
    "test_df = filtered_df[filtered_df.id.isin(test_ids)]\n",
    "val_df = filtered_df[filtered_df.id.isin(val_ids)]\n",
    "\n",
    "df_arrs = []\n",
    "for df in [train_df, test_df, val_df]:\n",
    "    df = df[['id', 'author']]\n",
    "\n",
    "    docs = []\n",
    "    docs_unavail_pg_ids = []\n",
    "    for pg_id in df.id:    \n",
    "        try:\n",
    "            doc = ' '.join(get_book(pg_id, os.path.join(tokens_dirname), level='tokens'))\n",
    "            docs.append(doc)\n",
    "        except:\n",
    "            docs_unavail_pg_ids.append(pg_id)\n",
    "    \n",
    "    df = df[~df.id.isin(docs_unavail_pg_ids)].reset_index()\n",
    "    df.insert(2, 'text', docs, True)\n",
    "    \n",
    "    df_arrs.append(df)\n",
    "\n",
    "train_df, test_df, val_df = df_arrs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
