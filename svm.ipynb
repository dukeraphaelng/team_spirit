{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ugabQ_mZeC9H",
        "4ufYe3avdgbw",
        "NI2fXvwxfQII"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Main Model**\n",
        "=="
      ],
      "metadata": {
        "id": "uhAMBfGclm0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vlukiyanov/pt-sdae"
      ],
      "metadata": {
        "id": "6m64Fg3bYQkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570ff78d-7fc6-452c-a39f-cadb6e0aab9c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pt-sdae'...\n",
            "remote: Enumerating objects: 323, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 323 (delta 0), reused 0 (delta 0), pack-reused 319\u001b[K\n",
            "Receiving objects: 100% (323/323), 78.64 KiB | 454.00 KiB/s, done.\n",
            "Resolving deltas: 100% (180/180), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pt-sdae"
      ],
      "metadata": {
        "id": "13S4rel3Z1OM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376d8e5a-7b3e-44d8-99e5-e31303d4366f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pt-sdae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "id": "PCjLKsOBYwX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca3c064-8e21-4888-c7ca-4e0dc8a4c425"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/setuptools/__init__.py:85: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated. Requirements should be satisfied by a PEP 517 installer. If you are using pip, you can try `pip install --use-pep517`.\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n",
            "running install\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating ptsdae.egg-info\n",
            "writing ptsdae.egg-info/PKG-INFO\n",
            "writing dependency_links to ptsdae.egg-info/dependency_links.txt\n",
            "writing requirements to ptsdae.egg-info/requires.txt\n",
            "writing top-level names to ptsdae.egg-info/top_level.txt\n",
            "writing manifest file 'ptsdae.egg-info/SOURCES.txt'\n",
            "reading manifest file 'ptsdae.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'ptsdae.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/ptsdae\n",
            "copying ptsdae/utils.py -> build/lib/ptsdae\n",
            "copying ptsdae/model.py -> build/lib/ptsdae\n",
            "copying ptsdae/sklearn_api.py -> build/lib/ptsdae\n",
            "copying ptsdae/sdae.py -> build/lib/ptsdae\n",
            "copying ptsdae/__init__.py -> build/lib/ptsdae\n",
            "copying ptsdae/dae.py -> build/lib/ptsdae\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/ptsdae\n",
            "copying build/lib/ptsdae/utils.py -> build/bdist.linux-x86_64/egg/ptsdae\n",
            "copying build/lib/ptsdae/model.py -> build/bdist.linux-x86_64/egg/ptsdae\n",
            "copying build/lib/ptsdae/sklearn_api.py -> build/bdist.linux-x86_64/egg/ptsdae\n",
            "copying build/lib/ptsdae/sdae.py -> build/bdist.linux-x86_64/egg/ptsdae\n",
            "copying build/lib/ptsdae/__init__.py -> build/bdist.linux-x86_64/egg/ptsdae\n",
            "copying build/lib/ptsdae/dae.py -> build/bdist.linux-x86_64/egg/ptsdae\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ptsdae/utils.py to utils.cpython-39.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ptsdae/model.py to model.cpython-39.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ptsdae/sklearn_api.py to sklearn_api.cpython-39.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ptsdae/sdae.py to sdae.cpython-39.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ptsdae/__init__.py to __init__.cpython-39.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ptsdae/dae.py to dae.cpython-39.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ptsdae.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ptsdae.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ptsdae.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ptsdae.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ptsdae.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/ptsdae-1.0-py3.9.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing ptsdae-1.0-py3.9.egg\n",
            "Copying ptsdae-1.0-py3.9.egg to /usr/local/lib/python3.9/dist-packages\n",
            "Adding ptsdae 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.9/dist-packages/ptsdae-1.0-py3.9.egg\n",
            "Processing dependencies for ptsdae==1.0\n",
            "Searching for cytoolz>=0.9.0.1\n",
            "Reading https://pypi.org/simple/cytoolz/\n",
            "Downloading https://files.pythonhosted.org/packages/34/72/da451bb5e9bada769a1ec2afdb9482756ed19369a725290143a73799c8e4/cytoolz-0.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl#sha256=2a48940ff0449ffcf690310bf9228bb57885f7571406ed2fe05c98e299987195\n",
            "Best match: cytoolz 0.12.1\n",
            "Processing cytoolz-0.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
            "Installing cytoolz-0.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl to /usr/local/lib/python3.9/dist-packages\n",
            "Adding cytoolz 0.12.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.9/dist-packages/cytoolz-0.12.1-py3.9-linux-x86_64.egg\n",
            "Searching for visdom>=0.1.05\n",
            "Reading https://pypi.org/simple/visdom/\n",
            "Downloading https://files.pythonhosted.org/packages/31/ab/6a8df57477ea6bb65b828f0b6725255982dfcd02f7ed353b895393616875/visdom-0.2.4.tar.gz#sha256=84a911d3c8814a056d54812b381bd938cb44bcfc503a85fe0f701502bb720574\n",
            "Best match: visdom 0.2.4\n",
            "Processing visdom-0.2.4.tar.gz\n",
            "Writing /tmp/easy_install-en9vrncv/visdom-0.2.4/setup.cfg\n",
            "Running visdom-0.2.4/setup.py -q bdist_egg --dist-dir /tmp/easy_install-en9vrncv/visdom-0.2.4/egg-dist-tmp-nqtly737\n",
            "warning: manifest_maker: MANIFEST.in, line 5: 'recursive-include' expects <dir> <pattern1> <pattern2> ...\n",
            "\n",
            "warning: no previously-included files matching '__pycache__' found under directory '*'\n",
            "warning: no previously-included files matching '*.py[co]' found under directory '*'\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'visdom.static' as data is deprecated, please list it in `packages`.\n",
            "    !!\n",
            "\n",
            "\n",
            "    ############################\n",
            "    # Package would be ignored #\n",
            "    ############################\n",
            "    Python recognizes 'visdom.static' as an importable package,\n",
            "    but it is not listed in the `packages` configuration of setuptools.\n",
            "\n",
            "    'visdom.static' has been automatically added to the distribution only\n",
            "    because it may contain data files, but this behavior is likely to change\n",
            "    in future versions of setuptools (and therefore is considered deprecated).\n",
            "\n",
            "    Please make sure that 'visdom.static' is included as a package by using\n",
            "    the `packages` configuration field or the proper discovery methods\n",
            "    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "    instead of `find_packages(...)`/`find:`).\n",
            "\n",
            "    You can read more about \"package discovery\" and \"data files\" on setuptools\n",
            "    documentation page.\n",
            "\n",
            "\n",
            "!!\n",
            "\n",
            "  check.warn(importable)\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'visdom.static.css' as data is deprecated, please list it in `packages`.\n",
            "    !!\n",
            "\n",
            "\n",
            "    ############################\n",
            "    # Package would be ignored #\n",
            "    ############################\n",
            "    Python recognizes 'visdom.static.css' as an importable package,\n",
            "    but it is not listed in the `packages` configuration of setuptools.\n",
            "\n",
            "    'visdom.static.css' has been automatically added to the distribution only\n",
            "    because it may contain data files, but this behavior is likely to change\n",
            "    in future versions of setuptools (and therefore is considered deprecated).\n",
            "\n",
            "    Please make sure that 'visdom.static.css' is included as a package by using\n",
            "    the `packages` configuration field or the proper discovery methods\n",
            "    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "    instead of `find_packages(...)`/`find:`).\n",
            "\n",
            "    You can read more about \"package discovery\" and \"data files\" on setuptools\n",
            "    documentation page.\n",
            "\n",
            "\n",
            "!!\n",
            "\n",
            "  check.warn(importable)\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'visdom.static.js' as data is deprecated, please list it in `packages`.\n",
            "    !!\n",
            "\n",
            "\n",
            "    ############################\n",
            "    # Package would be ignored #\n",
            "    ############################\n",
            "    Python recognizes 'visdom.static.js' as an importable package,\n",
            "    but it is not listed in the `packages` configuration of setuptools.\n",
            "\n",
            "    'visdom.static.js' has been automatically added to the distribution only\n",
            "    because it may contain data files, but this behavior is likely to change\n",
            "    in future versions of setuptools (and therefore is considered deprecated).\n",
            "\n",
            "    Please make sure that 'visdom.static.js' is included as a package by using\n",
            "    the `packages` configuration field or the proper discovery methods\n",
            "    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "    instead of `find_packages(...)`/`find:`).\n",
            "\n",
            "    You can read more about \"package discovery\" and \"data files\" on setuptools\n",
            "    documentation page.\n",
            "\n",
            "\n",
            "!!\n",
            "\n",
            "  check.warn(importable)\n",
            "creating /usr/local/lib/python3.9/dist-packages/visdom-0.2.4-py3.9.egg\n",
            "Extracting visdom-0.2.4-py3.9.egg to /usr/local/lib/python3.9/dist-packages\n",
            "Adding visdom 0.2.4 to easy-install.pth file\n",
            "Installing visdom script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.9/dist-packages/visdom-0.2.4-py3.9.egg\n",
            "Searching for jsonpatch\n",
            "Reading https://pypi.org/simple/jsonpatch/\n",
            "Downloading https://files.pythonhosted.org/packages/a3/55/f7c93bae36d869292aedfbcbae8b091386194874f16390d680136edd2b28/jsonpatch-1.32-py2.py3-none-any.whl#sha256=26ac385719ac9f54df8a2f0827bb8253aa3ea8ab7b3368457bcdb8c14595a397\n",
            "Best match: jsonpatch 1.32\n",
            "Processing jsonpatch-1.32-py2.py3-none-any.whl\n",
            "Installing jsonpatch-1.32-py2.py3-none-any.whl to /usr/local/lib/python3.9/dist-packages\n",
            "Adding jsonpatch 1.32 to easy-install.pth file\n",
            "Installing jsondiff script to /usr/local/bin\n",
            "Installing jsonpatch script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.9/dist-packages/jsonpatch-1.32-py3.9.egg\n",
            "Searching for jsonpointer>=1.9\n",
            "Reading https://pypi.org/simple/jsonpointer/\n",
            "Downloading https://files.pythonhosted.org/packages/a3/be/8dc9d31b50e38172c8020c40f497ce8debdb721545ddb9fcb7cca89ea9e6/jsonpointer-2.3-py2.py3-none-any.whl#sha256=51801e558539b4e9cd268638c078c6c5746c9ac96bc38152d443400e4f3793e9\n",
            "Best match: jsonpointer 2.3\n",
            "Processing jsonpointer-2.3-py2.py3-none-any.whl\n",
            "Installing jsonpointer-2.3-py2.py3-none-any.whl to /usr/local/lib/python3.9/dist-packages\n",
            "Adding jsonpointer 2.3 to easy-install.pth file\n",
            "Installing jsonpointer script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.9/dist-packages/jsonpointer-2.3-py3.9.egg\n",
            "Searching for scikit-learn==1.2.2\n",
            "Best match: scikit-learn 1.2.2\n",
            "Adding scikit-learn 1.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for tqdm==4.65.0\n",
            "Best match: tqdm 4.65.0\n",
            "Adding tqdm 4.65.0 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for xlrd==2.0.1\n",
            "Best match: xlrd 2.0.1\n",
            "Adding xlrd 2.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for click==8.1.3\n",
            "Best match: click 8.1.3\n",
            "Adding click 8.1.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for pandas==1.4.4\n",
            "Best match: pandas 1.4.4\n",
            "Adding pandas 1.4.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for scipy==1.10.1\n",
            "Best match: scipy 1.10.1\n",
            "Adding scipy 1.10.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for torch==2.0.0+cu118\n",
            "Best match: torch 2.0.0+cu118\n",
            "Adding torch 2.0.0+cu118 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for numpy==1.22.4\n",
            "Best match: numpy 1.22.4\n",
            "Adding numpy 1.22.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.9 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for threadpoolctl==3.1.0\n",
            "Best match: threadpoolctl 3.1.0\n",
            "Adding threadpoolctl 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for joblib==1.1.1\n",
            "Best match: joblib 1.1.1\n",
            "Adding joblib 1.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for toolz==0.12.0\n",
            "Best match: toolz 0.12.0\n",
            "Adding toolz 0.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for Pillow==8.4.0\n",
            "Best match: Pillow 8.4.0\n",
            "Adding Pillow 8.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for networkx==3.0\n",
            "Best match: networkx 3.0\n",
            "Adding networkx 3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for websocket-client==1.5.1\n",
            "Best match: websocket-client 1.5.1\n",
            "Adding websocket-client 1.5.1 to easy-install.pth file\n",
            "Installing wsdump script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for six==1.16.0\n",
            "Best match: six 1.16.0\n",
            "Adding six 1.16.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for tornado==6.2\n",
            "Best match: tornado 6.2\n",
            "Adding tornado 6.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for requests==2.27.1\n",
            "Best match: requests 2.27.1\n",
            "Adding requests 2.27.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for pytz==2022.7.1\n",
            "Best match: pytz 2022.7.1\n",
            "Adding pytz 2022.7.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for triton==2.0.0\n",
            "Best match: triton 2.0.0\n",
            "Adding triton 2.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for Jinja2==3.1.2\n",
            "Best match: Jinja2 3.1.2\n",
            "Adding Jinja2 3.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for sympy==1.11.1\n",
            "Best match: sympy 1.11.1\n",
            "Adding sympy 1.11.1 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for typing-extensions==4.5.0\n",
            "Best match: typing-extensions 4.5.0\n",
            "Adding typing-extensions 4.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for filelock==3.10.7\n",
            "Best match: filelock 3.10.7\n",
            "Adding filelock 3.10.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for idna==3.4\n",
            "Best match: idna 3.4\n",
            "Adding idna 3.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for charset-normalizer==2.0.12\n",
            "Best match: charset-normalizer 2.0.12\n",
            "Adding charset-normalizer 2.0.12 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for certifi==2022.12.7\n",
            "Best match: certifi 2022.12.7\n",
            "Adding certifi 2022.12.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for urllib3==1.26.15\n",
            "Best match: urllib3 1.26.15\n",
            "Adding urllib3 1.26.15 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for lit==16.0.0\n",
            "Best match: lit 16.0.0\n",
            "Adding lit 16.0.0 to easy-install.pth file\n",
            "Installing lit script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for cmake==3.25.2\n",
            "Best match: cmake 3.25.2\n",
            "Adding cmake 3.25.2 to easy-install.pth file\n",
            "Installing cmake script to /usr/local/bin\n",
            "Installing cpack script to /usr/local/bin\n",
            "Installing ctest script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for MarkupSafe==2.1.2\n",
            "Best match: MarkupSafe 2.1.2\n",
            "Adding MarkupSafe 2.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.9/dist-packages\n",
            "Finished processing dependencies for ptsdae==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall cytoolz"
      ],
      "metadata": {
        "id": "SNI8l5awe6np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ad03c9-3b98-4e5e-e4c5-c6adacd0b702"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: cytoolz 0.12.1\n",
            "Uninstalling cytoolz-0.12.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.9/dist-packages/cytoolz-0.12.1-py3.9-linux-x86_64.egg\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled cytoolz-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cytoolz"
      ],
      "metadata": {
        "id": "T0Nt9_jIewbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bde013e-4cf7-43cc-95a5-7e0bac87a61a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cytoolz\n",
            "  Downloading cytoolz-0.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz) (0.12.0)\n",
            "Installing collected packages: cytoolz\n",
            "Successfully installed cytoolz-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E3t7pILi0hfW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Classifier\n",
        "from sklearn.svm import SVC\n",
        "# Character N-gram feature extractor\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# SDAE and model\n",
        "from ptsdae.sdae import StackedDenoisingAutoEncoder\n",
        "from ptsdae import model as ae\n",
        "#from sdae import StackedDenoisingAE\n",
        "# Util\n",
        "from data_io import get_book\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training, test and validation sets"
      ],
      "metadata": {
        "id": "qp_i8Gm-29pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "val_data = pd.read_csv(\"val.csv\")\n",
        "\n",
        "cv = CountVectorizer(analyzer='char', ngram_range=(1, 3), stop_words='english', dtype=np.float32)\n",
        "X_train, X_test, X_val = cv.fit_transform(train_data.text.tolist()), cv.transform(test_data.text.tolist()), cv.transform(val_data.text.tolist())  \n",
        "Y_train, Y_test, Y_val = train_data.author.tolist(), test_data.author.tolist(), val_data.author.tolist()\n"
      ],
      "metadata": {
        "id": "nrEiMYo42xnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee64546d-9137-4182-de0e-21eb3d9b4a7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transform Y_train to one-hot encoded**"
      ],
      "metadata": {
        "id": "RYrS4WZ5wSj2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OxBbgLpxwRsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standardize datasets**"
      ],
      "metadata": {
        "id": "bmRydd11m3ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_test, X_val = X_train.toarray(), X_test.toarray(), X_val.toarray()\n",
        "\n",
        "scaler_train = StandardScaler()\n",
        "scaler_train.fit(X_train)\n",
        "X_train_scaled = torch.tensor(scaler_train.transform(X_train), dtype=torch.float32)\n",
        "\n",
        "X_test_scaled = torch.tensor(scaler_train.transform(X_test), dtype=torch.float32)\n",
        "\n",
        "X_val_scaled = torch.tensor(scaler_train.transform(X_val), dtype=torch.float32)\n",
        "\n",
        "# Add Gaussian noise\n",
        "#noise_factor = 0.1\n",
        "#X_train_noisy = X_train_scaled + noise_factor * np.random.normal(loc=0.0, scale=1., size=X_train.shape)\n",
        "\n",
        "print(X_train_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkhG5_rdkGFw",
        "outputId": "3caa9d03-346e-40c8-a3e3-56bf3aade7fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.4500,  1.4000,  1.2278,  ..., -0.1474, -0.1474, -0.1474],\n",
            "        [-0.5260, -0.5123, -0.6869,  ..., -0.1474, -0.1474, -0.1474],\n",
            "        [ 0.7717,  0.6023,  0.8595,  ..., -0.1474, -0.1474, -0.1474],\n",
            "        ...,\n",
            "        [ 0.3666,  0.4088,  0.3506,  ...,  6.7823,  6.7823,  6.7823],\n",
            "        [ 0.1063,  0.0084,  0.2834,  ..., -0.1474, -0.1474, -0.1474],\n",
            "        [ 0.1816,  0.0359,  0.1075,  ..., -0.1474, -0.1474, -0.1474]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple linear SVM classifier"
      ],
      "metadata": {
        "id": "EqcixcBw3EJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM classifier\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, Y_train)\n",
        "\n",
        "preds = svm.predict(X_test)\n",
        "\n",
        "print(Y_test)\n",
        "print(\"############################################\")\n",
        "print(preds)\n",
        "error = np.mean(preds != Y_test)\n",
        "print(\"Error of the computed SVM:\", error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1lyzx9p265L",
        "outputId": "cfab38ab-116d-42aa-992a-34fa5ae7fafd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Burnett, Frances Hodgson', 'London, Jack', 'Spinoza, Benedictus de', 'Shakespeare, William', 'Warner, Charles Dudley', 'Paine, Albert Bigelow', 'Howells, William Dean', 'Arthur, T. S. (Timothy Shay)', 'Churchill, Winston', 'Ebers, Georg', 'Twain, Mark', 'Holmes, Oliver Wendell', 'Lytton, Edward Bulwer Lytton, Baron', 'Molière', 'Whittier, John Greenleaf', 'Cicero, Marcus Tullius', 'Glyn, Elinor', 'Quiller-Couch, Arthur', 'Rinehart, Mary Roberts', 'Jacobs, W. W. (William Wymark)', 'Bennett, Arnold', 'Molesworth, Mrs.', 'Shoghi, Effendi', 'Ballantyne, R. M. (Robert Michael)', 'Rohmer, Sax', 'Lang, Andrew', 'Smith, Francis Hopkinson', 'Lowndes, Marie Belloc', 'Duncan, Norman', 'Locke, William John', 'Holt, Emily Sarah', 'Pemberton, Max', 'Burroughs, John', 'Reynolds, Mack', 'Wallace, F. L. (Floyd L.)', 'Barr, Amelia E.', 'Raymond, Evelyn', 'Hall, E. Raymond (Eugene Raymond)', 'Bangs, John Kendrick', 'Dewey, John', 'Roy, Lillian Elizabeth', 'Drake, Samuel Adams', 'Leinster, Murray', 'Alger, Horatio, Jr.', 'Phillpotts, Eden', 'Hawkins, N. (Nehemiah)', 'Mulford, Clarence Edward']\n",
            "############################################\n",
            "['Rinehart, Mary Roberts' 'Rinehart, Mary Roberts'\n",
            " 'Spinoza, Benedictus de' 'Molière' 'Holmes, Oliver Wendell'\n",
            " 'Lowndes, Marie Belloc' 'Drake, Samuel Adams' 'Burroughs, John'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Ebers, Georg'\n",
            " 'Howells, William Dean' 'Wallace, F. L. (Floyd L.)' 'Locke, William John'\n",
            " 'Ebers, Georg' 'Lytton, Edward Bulwer Lytton, Baron' 'Rohmer, Sax'\n",
            " 'Bennett, Arnold' 'Leinster, Murray' 'Lang, Andrew' 'Pemberton, Max'\n",
            " 'Raymond, Evelyn' 'Raymond, Evelyn' 'Rohmer, Sax' 'Churchill, Winston'\n",
            " 'Rohmer, Sax' 'Duncan, Norman' 'Smith, Francis Hopkinson'\n",
            " 'Roy, Lillian Elizabeth' 'Whittier, John Greenleaf'\n",
            " 'Mulford, Clarence Edward' 'Burnett, Frances Hodgson' 'Rohmer, Sax'\n",
            " 'Rohmer, Sax' 'Quiller-Couch, Arthur' 'Dewey, John'\n",
            " 'Roy, Lillian Elizabeth' 'Holt, Emily Sarah' 'Molière' 'Reynolds, Mack'\n",
            " 'Holmes, Oliver Wendell' 'Duncan, Norman' 'Holmes, Oliver Wendell'\n",
            " 'Duncan, Norman' 'Alger, Horatio, Jr.' 'Rinehart, Mary Roberts'\n",
            " 'Mulford, Clarence Edward' 'Lowndes, Marie Belloc']\n",
            "Error of the computed SVM: 0.8936170212765957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "zNkiuXq43IoJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SvmDataset(Dataset):\n",
        "    def __init__(self, train):\n",
        "        self.dataset = train\n",
        "        self._cache = dict()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index not in self._cache:\n",
        "            self._cache[index] = ((torch.tensor(self.dataset[index])), 1)\n",
        "            self._cache[index] = list(self._cache[index])\n",
        "            self._cache[index][0] = self._cache[index][0].cuda(non_blocking=True)\n",
        "            self._cache[index][1] = torch.tensor(self._cache[index][1]).cuda(\n",
        "                non_blocking=True\n",
        "            )\n",
        "        return self._cache[index]"
      ],
      "metadata": {
        "id": "Yux2TTpgjulb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(X_train.shape)\n",
        "autoencoder = StackedDenoisingAutoEncoder(\n",
        "        [X_train.shape[1], 500, 250, 50], final_activation=None\n",
        "    )\n",
        "X_train_data = SvmDataset(X_train_scaled)\n",
        "X_val_data = SvmDataset(X_val_scaled)\n",
        "\n",
        "print(X_train_data.__getitem__(0))\n",
        "print(X_val_data.__getitem__(0))\n",
        "\n",
        "print((X_train_data.__getitem__(0)[0]).shape)\n",
        "print((X_val_data.__getitem__(0)[0]).shape)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaF8VmpWQFSs",
        "outputId": "a4870904-c078-4cc9-e813-f88032a7690e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(47, 8762)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-432c1cd8f16b>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self._cache[index] = ((torch.tensor(self.dataset[index])), 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([ 1.4500,  1.4000,  1.2278,  ..., -0.1474, -0.1474, -0.1474],\n",
            "       device='cuda:0'), tensor(1, device='cuda:0')]\n",
            "[tensor([-0.8364, -0.8272, -0.8896,  ..., -0.1474, -0.1474, -0.1474],\n",
            "       device='cuda:0'), tensor(1, device='cuda:0')]\n",
            "torch.Size([8762])\n",
            "torch.Size([8762])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "autoencoder.cuda()\n",
        "print(\"Pretraining stage.\")\n",
        "ae.pretrain(\n",
        "    X_train_data,\n",
        "    autoencoder,\n",
        "    cuda=True,\n",
        "    validation=X_val_data,\n",
        "    epochs=100,\n",
        "    batch_size=50,\n",
        "    optimizer=lambda model: Adam(model.parameters()),\n",
        "    scheduler=lambda x: StepLR(x, 100, gamma=0.1),\n",
        "    corruption=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZWrpxiGbQLF",
        "outputId": "cbc49e32-a21c-4c1d-8a0c-354631966045"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretraining stage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?batch/s, epo=0, lss=0.000000, vls=-1.000000]<ipython-input-10-432c1cd8f16b>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self._cache[index] = ((torch.tensor(self.dataset[index])), 1)\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.40batch/s, epo=0, lss=1.505010, vls=-1.000000]\n",
            "100%|██████████| 1/1 [00:00<00:00, 137.02batch/s, epo=1, lss=1.780092, vls=37.549717]\n",
            "100%|██████████| 1/1 [00:00<00:00, 141.19batch/s, epo=2, lss=1.395969, vls=36.515934]\n",
            "100%|██████████| 1/1 [00:00<00:00, 154.54batch/s, epo=3, lss=1.268658, vls=36.564198]\n",
            "100%|██████████| 1/1 [00:00<00:00, 193.24batch/s, epo=4, lss=1.251801, vls=36.973938]\n",
            "100%|██████████| 1/1 [00:00<00:00, 175.52batch/s, epo=5, lss=1.186900, vls=37.343483]\n",
            "100%|██████████| 1/1 [00:00<00:00, 208.01batch/s, epo=6, lss=1.124614, vls=37.636120]\n",
            "100%|██████████| 1/1 [00:00<00:00, 204.71batch/s, epo=7, lss=1.071828, vls=37.816021]\n",
            "100%|██████████| 1/1 [00:00<00:00, 190.63batch/s, epo=8, lss=1.026487, vls=37.892056]\n",
            "100%|██████████| 1/1 [00:00<00:00, 208.53batch/s, epo=9, lss=0.960871, vls=37.882061]\n",
            "100%|██████████| 1/1 [00:00<00:00, 218.45batch/s, epo=10, lss=0.920986, vls=37.790913]\n",
            "100%|██████████| 1/1 [00:00<00:00, 223.72batch/s, epo=11, lss=0.874662, vls=37.654221]\n",
            "100%|██████████| 1/1 [00:00<00:00, 231.78batch/s, epo=12, lss=0.839709, vls=37.521736]\n",
            "100%|██████████| 1/1 [00:00<00:00, 151.64batch/s, epo=13, lss=0.801103, vls=37.383057]\n",
            "100%|██████████| 1/1 [00:00<00:00, 137.75batch/s, epo=14, lss=0.753705, vls=37.268452]\n",
            "100%|██████████| 1/1 [00:00<00:00, 103.35batch/s, epo=15, lss=0.727460, vls=37.221607]\n",
            "100%|██████████| 1/1 [00:00<00:00, 166.01batch/s, epo=16, lss=0.676991, vls=37.162830]\n",
            "100%|██████████| 1/1 [00:00<00:00, 129.24batch/s, epo=17, lss=0.638884, vls=37.135017]\n",
            "100%|██████████| 1/1 [00:00<00:00, 126.10batch/s, epo=18, lss=0.619055, vls=37.113525]\n",
            "100%|██████████| 1/1 [00:00<00:00, 170.58batch/s, epo=19, lss=0.600657, vls=37.077927]\n",
            "100%|██████████| 1/1 [00:00<00:00, 126.51batch/s, epo=20, lss=0.559708, vls=37.052124]\n",
            "100%|██████████| 1/1 [00:00<00:00, 207.18batch/s, epo=21, lss=0.532560, vls=37.024590]\n",
            "100%|██████████| 1/1 [00:00<00:00, 197.70batch/s, epo=22, lss=0.512411, vls=36.966698]\n",
            "100%|██████████| 1/1 [00:00<00:00, 193.87batch/s, epo=23, lss=0.470893, vls=36.887749]\n",
            "100%|██████████| 1/1 [00:00<00:00, 195.40batch/s, epo=24, lss=0.470066, vls=36.857307]\n",
            "100%|██████████| 1/1 [00:00<00:00, 219.68batch/s, epo=25, lss=0.438976, vls=36.752121]\n",
            "100%|██████████| 1/1 [00:00<00:00, 153.58batch/s, epo=26, lss=0.424998, vls=36.688137]\n",
            "100%|██████████| 1/1 [00:00<00:00, 248.23batch/s, epo=27, lss=0.398069, vls=36.657974]\n",
            "100%|██████████| 1/1 [00:00<00:00, 173.63batch/s, epo=28, lss=0.385758, vls=36.642319]\n",
            "100%|██████████| 1/1 [00:00<00:00, 202.30batch/s, epo=29, lss=0.364115, vls=36.593948]\n",
            "100%|██████████| 1/1 [00:00<00:00, 188.50batch/s, epo=30, lss=0.339304, vls=36.556892]\n",
            "100%|██████████| 1/1 [00:00<00:00, 147.23batch/s, epo=31, lss=0.380245, vls=36.525063]\n",
            "100%|██████████| 1/1 [00:00<00:00, 180.35batch/s, epo=32, lss=0.330027, vls=36.483269]\n",
            "100%|██████████| 1/1 [00:00<00:00, 254.20batch/s, epo=33, lss=0.323376, vls=36.471840]\n",
            "100%|██████████| 1/1 [00:00<00:00, 201.05batch/s, epo=34, lss=0.338883, vls=36.473576]\n",
            "100%|██████████| 1/1 [00:00<00:00, 256.99batch/s, epo=35, lss=0.313183, vls=36.439537]\n",
            "100%|██████████| 1/1 [00:00<00:00, 243.97batch/s, epo=36, lss=0.284660, vls=36.401699]\n",
            "100%|██████████| 1/1 [00:00<00:00, 252.62batch/s, epo=37, lss=0.276797, vls=36.376572]\n",
            "100%|██████████| 1/1 [00:00<00:00, 200.81batch/s, epo=38, lss=0.265315, vls=36.352165]\n",
            "100%|██████████| 1/1 [00:00<00:00, 200.21batch/s, epo=39, lss=0.258342, vls=36.325371]\n",
            "100%|██████████| 1/1 [00:00<00:00, 102.51batch/s, epo=40, lss=0.248584, vls=36.353130]\n",
            "100%|██████████| 1/1 [00:00<00:00, 144.59batch/s, epo=41, lss=0.231951, vls=36.425392]\n",
            "100%|██████████| 1/1 [00:00<00:00, 122.25batch/s, epo=42, lss=0.228110, vls=36.468735]\n",
            "100%|██████████| 1/1 [00:00<00:00, 241.27batch/s, epo=43, lss=0.222354, vls=36.495125]\n",
            "100%|██████████| 1/1 [00:00<00:00, 148.26batch/s, epo=44, lss=0.216089, vls=36.539288]\n",
            "100%|██████████| 1/1 [00:00<00:00, 163.45batch/s, epo=45, lss=0.217010, vls=36.499134]\n",
            "100%|██████████| 1/1 [00:00<00:00, 114.85batch/s, epo=46, lss=0.198534, vls=36.438236]\n",
            "100%|██████████| 1/1 [00:00<00:00, 144.42batch/s, epo=47, lss=0.227260, vls=36.433960]\n",
            "100%|██████████| 1/1 [00:00<00:00, 200.80batch/s, epo=48, lss=0.216455, vls=36.426628]\n",
            "100%|██████████| 1/1 [00:00<00:00, 202.83batch/s, epo=49, lss=0.197366, vls=36.404251]\n",
            "100%|██████████| 1/1 [00:00<00:00, 209.27batch/s, epo=50, lss=0.201803, vls=36.410831]\n",
            "100%|██████████| 1/1 [00:00<00:00, 224.16batch/s, epo=51, lss=0.187454, vls=36.447746]\n",
            "100%|██████████| 1/1 [00:00<00:00, 210.98batch/s, epo=52, lss=0.199248, vls=36.475941]\n",
            "100%|██████████| 1/1 [00:00<00:00, 249.41batch/s, epo=53, lss=0.203480, vls=36.481209]\n",
            "100%|██████████| 1/1 [00:00<00:00, 210.99batch/s, epo=54, lss=0.182846, vls=36.480476]\n",
            "100%|██████████| 1/1 [00:00<00:00, 217.27batch/s, epo=55, lss=0.179670, vls=36.471790]\n",
            "100%|██████████| 1/1 [00:00<00:00, 268.80batch/s, epo=56, lss=0.192197, vls=36.458317]\n",
            "100%|██████████| 1/1 [00:00<00:00, 252.88batch/s, epo=57, lss=0.183321, vls=36.445324]\n",
            "100%|██████████| 1/1 [00:00<00:00, 280.01batch/s, epo=58, lss=0.158600, vls=36.492840]\n",
            "100%|██████████| 1/1 [00:00<00:00, 194.22batch/s, epo=59, lss=0.177703, vls=36.529076]\n",
            "100%|██████████| 1/1 [00:00<00:00, 243.81batch/s, epo=60, lss=0.158890, vls=36.480515]\n",
            "100%|██████████| 1/1 [00:00<00:00, 263.33batch/s, epo=61, lss=0.151713, vls=36.439667]\n",
            "100%|██████████| 1/1 [00:00<00:00, 246.61batch/s, epo=62, lss=0.166872, vls=36.455044]\n",
            "100%|██████████| 1/1 [00:00<00:00, 249.25batch/s, epo=63, lss=0.162629, vls=36.450848]\n",
            "100%|██████████| 1/1 [00:00<00:00, 183.69batch/s, epo=64, lss=0.187807, vls=36.413197]\n",
            "100%|██████████| 1/1 [00:00<00:00, 231.15batch/s, epo=65, lss=0.150911, vls=36.473595]\n",
            "100%|██████████| 1/1 [00:00<00:00, 199.48batch/s, epo=66, lss=0.158700, vls=36.524200]\n",
            "100%|██████████| 1/1 [00:00<00:00, 251.79batch/s, epo=67, lss=0.167862, vls=36.496788]\n",
            "100%|██████████| 1/1 [00:00<00:00, 240.00batch/s, epo=68, lss=0.151986, vls=36.498196]\n",
            "100%|██████████| 1/1 [00:00<00:00, 251.41batch/s, epo=69, lss=0.138962, vls=36.497322]\n",
            "100%|██████████| 1/1 [00:00<00:00, 247.38batch/s, epo=70, lss=0.169931, vls=36.481171]\n",
            "100%|██████████| 1/1 [00:00<00:00, 117.56batch/s, epo=71, lss=0.147331, vls=36.445164]\n",
            "100%|██████████| 1/1 [00:00<00:00, 167.69batch/s, epo=72, lss=0.152069, vls=36.427528]\n",
            "100%|██████████| 1/1 [00:00<00:00, 236.31batch/s, epo=73, lss=0.140971, vls=36.446423]\n",
            "100%|██████████| 1/1 [00:00<00:00, 128.97batch/s, epo=74, lss=0.165370, vls=36.487850]\n",
            "100%|██████████| 1/1 [00:00<00:00, 248.32batch/s, epo=75, lss=0.142016, vls=36.403736]\n",
            "100%|██████████| 1/1 [00:00<00:00, 134.75batch/s, epo=76, lss=0.171012, vls=36.397068]\n",
            "100%|██████████| 1/1 [00:00<00:00, 225.48batch/s, epo=77, lss=0.152715, vls=36.417492]\n",
            "100%|██████████| 1/1 [00:00<00:00, 160.71batch/s, epo=78, lss=0.153782, vls=36.407860]\n",
            "100%|██████████| 1/1 [00:00<00:00, 238.45batch/s, epo=79, lss=0.152258, vls=36.410770]\n",
            "100%|██████████| 1/1 [00:00<00:00, 165.60batch/s, epo=80, lss=0.158976, vls=36.458961]\n",
            "100%|██████████| 1/1 [00:00<00:00, 235.58batch/s, epo=81, lss=0.130624, vls=36.480591]\n",
            "100%|██████████| 1/1 [00:00<00:00, 249.97batch/s, epo=82, lss=0.195700, vls=36.475380]\n",
            "100%|██████████| 1/1 [00:00<00:00, 253.65batch/s, epo=83, lss=0.144491, vls=36.461025]\n",
            "100%|██████████| 1/1 [00:00<00:00, 233.64batch/s, epo=84, lss=0.152726, vls=36.467644]\n",
            "100%|██████████| 1/1 [00:00<00:00, 243.02batch/s, epo=85, lss=0.153702, vls=36.481697]\n",
            "100%|██████████| 1/1 [00:00<00:00, 245.76batch/s, epo=86, lss=0.152039, vls=36.437595]\n",
            "100%|██████████| 1/1 [00:00<00:00, 228.92batch/s, epo=87, lss=0.133702, vls=36.422138]\n",
            "100%|██████████| 1/1 [00:00<00:00, 189.59batch/s, epo=88, lss=0.155003, vls=36.404579]\n",
            "100%|██████████| 1/1 [00:00<00:00, 241.55batch/s, epo=89, lss=0.130394, vls=36.362923]\n",
            "100%|██████████| 1/1 [00:00<00:00, 154.63batch/s, epo=90, lss=0.139495, vls=36.352753]\n",
            "100%|██████████| 1/1 [00:00<00:00, 212.52batch/s, epo=91, lss=0.134735, vls=36.358704]\n",
            "100%|██████████| 1/1 [00:00<00:00, 165.68batch/s, epo=92, lss=0.152920, vls=36.360588]\n",
            "100%|██████████| 1/1 [00:00<00:00, 239.76batch/s, epo=93, lss=0.137048, vls=36.325943]\n",
            "100%|██████████| 1/1 [00:00<00:00, 241.23batch/s, epo=94, lss=0.138865, vls=36.292130]\n",
            "100%|██████████| 1/1 [00:00<00:00, 231.90batch/s, epo=95, lss=0.129795, vls=36.269962]\n",
            "100%|██████████| 1/1 [00:00<00:00, 178.93batch/s, epo=96, lss=0.135387, vls=36.227783]\n",
            "100%|██████████| 1/1 [00:00<00:00, 138.28batch/s, epo=97, lss=0.145251, vls=36.160419]\n",
            "100%|██████████| 1/1 [00:00<00:00, 235.48batch/s, epo=98, lss=0.133849, vls=36.113850]\n",
            "100%|██████████| 1/1 [00:00<00:00, 222.83batch/s, epo=99, lss=0.124999, vls=36.065056]\n",
            "100%|██████████| 1/1 [00:00<00:00, 149.07batch/s, epo=0, lss=5.310219, vls=-1.000000]\n",
            "100%|██████████| 1/1 [00:00<00:00, 229.60batch/s, epo=1, lss=4.078780, vls=140.107269]\n",
            "100%|██████████| 1/1 [00:00<00:00, 213.29batch/s, epo=2, lss=3.451505, vls=113.151260]\n",
            "100%|██████████| 1/1 [00:00<00:00, 239.16batch/s, epo=3, lss=2.935451, vls=96.050087]\n",
            "100%|██████████| 1/1 [00:00<00:00, 276.61batch/s, epo=4, lss=2.569961, vls=85.070213]\n",
            "100%|██████████| 1/1 [00:00<00:00, 279.01batch/s, epo=5, lss=2.218337, vls=77.800743]\n",
            "100%|██████████| 1/1 [00:00<00:00, 314.93batch/s, epo=6, lss=2.071439, vls=72.859795]\n",
            "100%|██████████| 1/1 [00:00<00:00, 300.34batch/s, epo=7, lss=1.891390, vls=69.454033]\n",
            "100%|██████████| 1/1 [00:00<00:00, 283.76batch/s, epo=8, lss=1.773510, vls=66.999886]\n",
            "100%|██████████| 1/1 [00:00<00:00, 287.60batch/s, epo=9, lss=1.692361, vls=65.217896]\n",
            "100%|██████████| 1/1 [00:00<00:00, 337.92batch/s, epo=10, lss=1.634445, vls=63.913658]\n",
            "100%|██████████| 1/1 [00:00<00:00, 288.21batch/s, epo=11, lss=1.540576, vls=62.810497]\n",
            "100%|██████████| 1/1 [00:00<00:00, 158.51batch/s, epo=12, lss=1.489043, vls=61.916836]\n",
            "100%|██████████| 1/1 [00:00<00:00, 106.61batch/s, epo=13, lss=1.456488, vls=61.206646]\n",
            "100%|██████████| 1/1 [00:00<00:00, 208.39batch/s, epo=14, lss=1.428807, vls=60.641117]\n",
            "100%|██████████| 1/1 [00:00<00:00, 247.54batch/s, epo=15, lss=1.385998, vls=60.141468]\n",
            "100%|██████████| 1/1 [00:00<00:00, 286.85batch/s, epo=16, lss=1.366317, vls=59.720047]\n",
            "100%|██████████| 1/1 [00:00<00:00, 293.76batch/s, epo=17, lss=1.367740, vls=59.375607]\n",
            "100%|██████████| 1/1 [00:00<00:00, 51.86batch/s, epo=18, lss=1.312748, vls=59.080910]\n",
            "100%|██████████| 1/1 [00:00<00:00, 271.44batch/s, epo=19, lss=1.289691, vls=58.809128]\n",
            "100%|██████████| 1/1 [00:00<00:00, 291.13batch/s, epo=20, lss=1.253880, vls=58.535370]\n",
            "100%|██████████| 1/1 [00:00<00:00, 267.90batch/s, epo=21, lss=1.252832, vls=58.248909]\n",
            "100%|██████████| 1/1 [00:00<00:00, 258.06batch/s, epo=22, lss=1.249895, vls=57.938477]\n",
            "100%|██████████| 1/1 [00:00<00:00, 281.82batch/s, epo=23, lss=1.202643, vls=57.598976]\n",
            "100%|██████████| 1/1 [00:00<00:00, 117.49batch/s, epo=24, lss=1.200905, vls=57.248550]\n",
            "100%|██████████| 1/1 [00:00<00:00, 211.16batch/s, epo=25, lss=1.181655, vls=56.870731]\n",
            "100%|██████████| 1/1 [00:00<00:00, 185.87batch/s, epo=26, lss=1.163085, vls=56.480381]\n",
            "100%|██████████| 1/1 [00:00<00:00, 223.30batch/s, epo=27, lss=1.133946, vls=56.086945]\n",
            "100%|██████████| 1/1 [00:00<00:00, 292.96batch/s, epo=28, lss=1.122719, vls=55.675373]\n",
            "100%|██████████| 1/1 [00:00<00:00, 275.72batch/s, epo=29, lss=1.107816, vls=55.268265]\n",
            "100%|██████████| 1/1 [00:00<00:00, 270.22batch/s, epo=30, lss=1.093024, vls=54.866280]\n",
            "100%|██████████| 1/1 [00:00<00:00, 291.19batch/s, epo=31, lss=1.068372, vls=54.459675]\n",
            "100%|██████████| 1/1 [00:00<00:00, 278.88batch/s, epo=32, lss=1.041223, vls=54.070595]\n",
            "100%|██████████| 1/1 [00:00<00:00, 320.71batch/s, epo=33, lss=1.036481, vls=53.707977]\n",
            "100%|██████████| 1/1 [00:00<00:00, 334.50batch/s, epo=34, lss=0.990317, vls=53.388531]\n",
            "100%|██████████| 1/1 [00:00<00:00, 288.15batch/s, epo=35, lss=1.019092, vls=53.119381]\n",
            "100%|██████████| 1/1 [00:00<00:00, 273.55batch/s, epo=36, lss=0.979814, vls=52.885777]\n",
            "100%|██████████| 1/1 [00:00<00:00, 272.62batch/s, epo=37, lss=0.983925, vls=52.696766]\n",
            "100%|██████████| 1/1 [00:00<00:00, 300.00batch/s, epo=38, lss=0.935336, vls=52.532032]\n",
            "100%|██████████| 1/1 [00:00<00:00, 303.03batch/s, epo=39, lss=0.941757, vls=52.353027]\n",
            "100%|██████████| 1/1 [00:00<00:00, 294.50batch/s, epo=40, lss=0.896827, vls=52.175056]\n",
            "100%|██████████| 1/1 [00:00<00:00, 297.57batch/s, epo=41, lss=0.908598, vls=51.991920]\n",
            "100%|██████████| 1/1 [00:00<00:00, 292.88batch/s, epo=42, lss=0.889743, vls=51.776237]\n",
            "100%|██████████| 1/1 [00:00<00:00, 310.14batch/s, epo=43, lss=0.861603, vls=51.581139]\n",
            "100%|██████████| 1/1 [00:00<00:00, 295.23batch/s, epo=44, lss=0.875058, vls=51.421673]\n",
            "100%|██████████| 1/1 [00:00<00:00, 302.68batch/s, epo=45, lss=0.841048, vls=51.267094]\n",
            "100%|██████████| 1/1 [00:00<00:00, 285.48batch/s, epo=46, lss=0.837677, vls=51.129120]\n",
            "100%|██████████| 1/1 [00:00<00:00, 265.19batch/s, epo=47, lss=0.854679, vls=50.993668]\n",
            "100%|██████████| 1/1 [00:00<00:00, 269.26batch/s, epo=48, lss=0.841096, vls=50.850441]\n",
            "100%|██████████| 1/1 [00:00<00:00, 268.09batch/s, epo=49, lss=0.821552, vls=50.720615]\n",
            "100%|██████████| 1/1 [00:00<00:00, 295.04batch/s, epo=50, lss=0.779341, vls=50.619572]\n",
            "100%|██████████| 1/1 [00:00<00:00, 298.63batch/s, epo=51, lss=0.797718, vls=50.539318]\n",
            "100%|██████████| 1/1 [00:00<00:00, 274.87batch/s, epo=52, lss=0.785303, vls=50.477543]\n",
            "100%|██████████| 1/1 [00:00<00:00, 305.42batch/s, epo=53, lss=0.746741, vls=50.420307]\n",
            "100%|██████████| 1/1 [00:00<00:00, 164.31batch/s, epo=54, lss=0.782577, vls=50.346325]\n",
            "100%|██████████| 1/1 [00:00<00:00, 199.09batch/s, epo=55, lss=0.743853, vls=50.284100]\n",
            "100%|██████████| 1/1 [00:00<00:00, 363.30batch/s, epo=56, lss=0.769235, vls=50.247952]\n",
            "100%|██████████| 1/1 [00:00<00:00, 345.49batch/s, epo=57, lss=0.752722, vls=50.207870]\n",
            "100%|██████████| 1/1 [00:00<00:00, 162.37batch/s, epo=58, lss=0.724323, vls=50.178570]\n",
            "100%|██████████| 1/1 [00:00<00:00, 312.84batch/s, epo=59, lss=0.764962, vls=50.137352]\n",
            "100%|██████████| 1/1 [00:00<00:00, 199.11batch/s, epo=60, lss=0.724974, vls=50.083702]\n",
            "100%|██████████| 1/1 [00:00<00:00, 178.20batch/s, epo=61, lss=0.708919, vls=50.026520]\n",
            "100%|██████████| 1/1 [00:00<00:00, 318.98batch/s, epo=62, lss=0.730617, vls=49.954548]\n",
            "100%|██████████| 1/1 [00:00<00:00, 329.77batch/s, epo=63, lss=0.706009, vls=49.879791]\n",
            "100%|██████████| 1/1 [00:00<00:00, 297.47batch/s, epo=64, lss=0.660231, vls=49.815510]\n",
            "100%|██████████| 1/1 [00:00<00:00, 277.60batch/s, epo=65, lss=0.680277, vls=49.763325]\n",
            "100%|██████████| 1/1 [00:00<00:00, 292.43batch/s, epo=66, lss=0.683774, vls=49.727745]\n",
            "100%|██████████| 1/1 [00:00<00:00, 297.38batch/s, epo=67, lss=0.652405, vls=49.717583]\n",
            "100%|██████████| 1/1 [00:00<00:00, 327.55batch/s, epo=68, lss=0.677451, vls=49.710403]\n",
            "100%|██████████| 1/1 [00:00<00:00, 347.73batch/s, epo=69, lss=0.639903, vls=49.728802]\n",
            "100%|██████████| 1/1 [00:00<00:00, 316.89batch/s, epo=70, lss=0.665778, vls=49.737637]\n",
            "100%|██████████| 1/1 [00:00<00:00, 316.67batch/s, epo=71, lss=0.654312, vls=49.737286]\n",
            "100%|██████████| 1/1 [00:00<00:00, 158.89batch/s, epo=72, lss=0.645627, vls=49.735790]\n",
            "100%|██████████| 1/1 [00:00<00:00, 302.88batch/s, epo=73, lss=0.628988, vls=49.725349]\n",
            "100%|██████████| 1/1 [00:00<00:00, 272.61batch/s, epo=74, lss=0.638550, vls=49.700787]\n",
            "100%|██████████| 1/1 [00:00<00:00, 276.92batch/s, epo=75, lss=0.625363, vls=49.663200]\n",
            "100%|██████████| 1/1 [00:00<00:00, 326.46batch/s, epo=76, lss=0.617113, vls=49.644543]\n",
            "100%|██████████| 1/1 [00:00<00:00, 323.16batch/s, epo=77, lss=0.628614, vls=49.653973]\n",
            "100%|██████████| 1/1 [00:00<00:00, 352.02batch/s, epo=78, lss=0.635069, vls=49.680286]\n",
            "100%|██████████| 1/1 [00:00<00:00, 358.12batch/s, epo=79, lss=0.567427, vls=49.724552]\n",
            "100%|██████████| 1/1 [00:00<00:00, 119.36batch/s, epo=80, lss=0.597975, vls=49.726089]\n",
            "100%|██████████| 1/1 [00:00<00:00, 314.86batch/s, epo=81, lss=0.592590, vls=49.673637]\n",
            "100%|██████████| 1/1 [00:00<00:00, 311.31batch/s, epo=82, lss=0.556747, vls=49.618622]\n",
            "100%|██████████| 1/1 [00:00<00:00, 371.93batch/s, epo=83, lss=0.562496, vls=49.587902]\n",
            "100%|██████████| 1/1 [00:00<00:00, 319.52batch/s, epo=84, lss=0.587856, vls=49.547211]\n",
            "100%|██████████| 1/1 [00:00<00:00, 221.03batch/s, epo=85, lss=0.553605, vls=49.497009]\n",
            "100%|██████████| 1/1 [00:00<00:00, 292.59batch/s, epo=86, lss=0.568583, vls=49.482101]\n",
            "100%|██████████| 1/1 [00:00<00:00, 313.94batch/s, epo=87, lss=0.570855, vls=49.501186]\n",
            "100%|██████████| 1/1 [00:00<00:00, 333.86batch/s, epo=88, lss=0.550335, vls=49.505829]\n",
            "100%|██████████| 1/1 [00:00<00:00, 289.98batch/s, epo=89, lss=0.565432, vls=49.506435]\n",
            "100%|██████████| 1/1 [00:00<00:00, 116.52batch/s, epo=90, lss=0.529913, vls=49.501457]\n",
            "100%|██████████| 1/1 [00:00<00:00, 341.97batch/s, epo=91, lss=0.549215, vls=49.501781]\n",
            "100%|██████████| 1/1 [00:00<00:00, 314.13batch/s, epo=92, lss=0.536499, vls=49.500336]\n",
            "100%|██████████| 1/1 [00:00<00:00, 307.79batch/s, epo=93, lss=0.526796, vls=49.494354]\n",
            "100%|██████████| 1/1 [00:00<00:00, 373.32batch/s, epo=94, lss=0.495558, vls=49.483143]\n",
            "100%|██████████| 1/1 [00:00<00:00, 329.48batch/s, epo=95, lss=0.503346, vls=49.493786]\n",
            "100%|██████████| 1/1 [00:00<00:00, 291.74batch/s, epo=96, lss=0.474081, vls=49.498211]\n",
            "100%|██████████| 1/1 [00:00<00:00, 71.36batch/s, epo=97, lss=0.533399, vls=49.490639]\n",
            "100%|██████████| 1/1 [00:00<00:00, 130.28batch/s, epo=98, lss=0.574189, vls=49.489399]\n",
            "100%|██████████| 1/1 [00:00<00:00, 205.11batch/s, epo=99, lss=0.516246, vls=49.497032]\n",
            "100%|██████████| 1/1 [00:00<00:00, 208.16batch/s, epo=0, lss=1.336087, vls=-1.000000]\n",
            "100%|██████████| 1/1 [00:00<00:00, 203.50batch/s, epo=1, lss=1.241476, vls=31.714266]\n",
            "100%|██████████| 1/1 [00:00<00:00, 237.26batch/s, epo=2, lss=1.157305, vls=30.594114]\n",
            "100%|██████████| 1/1 [00:00<00:00, 246.83batch/s, epo=3, lss=1.082493, vls=29.602621]\n",
            "100%|██████████| 1/1 [00:00<00:00, 230.65batch/s, epo=4, lss=1.015966, vls=28.725245]\n",
            "100%|██████████| 1/1 [00:00<00:00, 304.13batch/s, epo=5, lss=0.956710, vls=27.947851]\n",
            "100%|██████████| 1/1 [00:00<00:00, 305.31batch/s, epo=6, lss=0.903801, vls=27.257292]\n",
            "100%|██████████| 1/1 [00:00<00:00, 269.21batch/s, epo=7, lss=0.856419, vls=26.641981]\n",
            "100%|██████████| 1/1 [00:00<00:00, 263.88batch/s, epo=8, lss=0.813844, vls=26.091692]\n",
            "100%|██████████| 1/1 [00:00<00:00, 255.66batch/s, epo=9, lss=0.775449, vls=25.597624]\n",
            "100%|██████████| 1/1 [00:00<00:00, 250.50batch/s, epo=10, lss=0.740692, vls=25.152319]\n",
            "100%|██████████| 1/1 [00:00<00:00, 218.52batch/s, epo=11, lss=0.709103, vls=24.749367]\n",
            "100%|██████████| 1/1 [00:00<00:00, 308.36batch/s, epo=12, lss=0.680278, vls=24.383202]\n",
            "100%|██████████| 1/1 [00:00<00:00, 305.66batch/s, epo=13, lss=0.653866, vls=24.048920]\n",
            "100%|██████████| 1/1 [00:00<00:00, 239.06batch/s, epo=14, lss=0.629567, vls=23.742205]\n",
            "100%|██████████| 1/1 [00:00<00:00, 230.60batch/s, epo=15, lss=0.607121, vls=23.459244]\n",
            "100%|██████████| 1/1 [00:00<00:00, 267.15batch/s, epo=16, lss=0.586304, vls=23.196749]\n",
            "100%|██████████| 1/1 [00:00<00:00, 224.59batch/s, epo=17, lss=0.566924, vls=22.951891]\n",
            "100%|██████████| 1/1 [00:00<00:00, 230.98batch/s, epo=18, lss=0.548819, vls=22.722300]\n",
            "100%|██████████| 1/1 [00:00<00:00, 218.39batch/s, epo=19, lss=0.531849, vls=22.506012]\n",
            "100%|██████████| 1/1 [00:00<00:00, 230.36batch/s, epo=20, lss=0.515895, vls=22.301434]\n",
            "100%|██████████| 1/1 [00:00<00:00, 232.35batch/s, epo=21, lss=0.500858, vls=22.107254]\n",
            "100%|██████████| 1/1 [00:00<00:00, 238.49batch/s, epo=22, lss=0.486649, vls=21.922422]\n",
            "100%|██████████| 1/1 [00:00<00:00, 270.51batch/s, epo=23, lss=0.473194, vls=21.746063]\n",
            "100%|██████████| 1/1 [00:00<00:00, 214.51batch/s, epo=24, lss=0.460424, vls=21.577450]\n",
            "100%|██████████| 1/1 [00:00<00:00, 291.17batch/s, epo=25, lss=0.448280, vls=21.415947]\n",
            "100%|██████████| 1/1 [00:00<00:00, 249.13batch/s, epo=26, lss=0.436707, vls=21.260998]\n",
            "100%|██████████| 1/1 [00:00<00:00, 171.45batch/s, epo=27, lss=0.425658, vls=21.112099]\n",
            "100%|██████████| 1/1 [00:00<00:00, 117.54batch/s, epo=28, lss=0.415088, vls=20.968782]\n",
            "100%|██████████| 1/1 [00:00<00:00, 176.30batch/s, epo=29, lss=0.404960, vls=20.830618]\n",
            "100%|██████████| 1/1 [00:00<00:00, 363.49batch/s, epo=30, lss=0.395238, vls=20.697199]\n",
            "100%|██████████| 1/1 [00:00<00:00, 341.78batch/s, epo=31, lss=0.385893, vls=20.568123]\n",
            "100%|██████████| 1/1 [00:00<00:00, 341.22batch/s, epo=32, lss=0.376898, vls=20.443007]\n",
            "100%|██████████| 1/1 [00:00<00:00, 178.82batch/s, epo=33, lss=0.368230, vls=20.321478]\n",
            "100%|██████████| 1/1 [00:00<00:00, 225.52batch/s, epo=34, lss=0.359868, vls=20.203196]\n",
            "100%|██████████| 1/1 [00:00<00:00, 350.55batch/s, epo=35, lss=0.351792, vls=20.087856]\n",
            "100%|██████████| 1/1 [00:00<00:00, 371.57batch/s, epo=36, lss=0.343984, vls=19.975208]\n",
            "100%|██████████| 1/1 [00:00<00:00, 437.32batch/s, epo=37, lss=0.336429, vls=19.865063]\n",
            "100%|██████████| 1/1 [00:00<00:00, 365.87batch/s, epo=38, lss=0.329113, vls=19.757299]\n",
            "100%|██████████| 1/1 [00:00<00:00, 414.42batch/s, epo=39, lss=0.322024, vls=19.651869]\n",
            "100%|██████████| 1/1 [00:00<00:00, 344.76batch/s, epo=40, lss=0.315150, vls=19.548782]\n",
            "100%|██████████| 1/1 [00:00<00:00, 116.27batch/s, epo=41, lss=0.308482, vls=19.448093]\n",
            "100%|██████████| 1/1 [00:00<00:00, 363.05batch/s, epo=42, lss=0.302014, vls=19.349903]\n",
            "100%|██████████| 1/1 [00:00<00:00, 383.08batch/s, epo=43, lss=0.295737, vls=19.254326]\n",
            "100%|██████████| 1/1 [00:00<00:00, 364.15batch/s, epo=44, lss=0.289647, vls=19.161478]\n",
            "100%|██████████| 1/1 [00:00<00:00, 356.93batch/s, epo=45, lss=0.283738, vls=19.071459]\n",
            "100%|██████████| 1/1 [00:00<00:00, 272.96batch/s, epo=46, lss=0.278006, vls=18.984343]\n",
            "100%|██████████| 1/1 [00:00<00:00, 380.50batch/s, epo=47, lss=0.272446, vls=18.900169]\n",
            "100%|██████████| 1/1 [00:00<00:00, 362.20batch/s, epo=48, lss=0.267055, vls=18.818930]\n",
            "100%|██████████| 1/1 [00:00<00:00, 376.98batch/s, epo=49, lss=0.261828, vls=18.740585]\n",
            "100%|██████████| 1/1 [00:00<00:00, 382.55batch/s, epo=50, lss=0.256761, vls=18.665052]\n",
            "100%|██████████| 1/1 [00:00<00:00, 421.58batch/s, epo=51, lss=0.251851, vls=18.592220]\n",
            "100%|██████████| 1/1 [00:00<00:00, 174.89batch/s, epo=52, lss=0.247092, vls=18.521950]\n",
            "100%|██████████| 1/1 [00:00<00:00, 158.74batch/s, epo=53, lss=0.242483, vls=18.454096]\n",
            "100%|██████████| 1/1 [00:00<00:00, 364.34batch/s, epo=54, lss=0.238018, vls=18.388500]\n",
            "100%|██████████| 1/1 [00:00<00:00, 331.23batch/s, epo=55, lss=0.233694, vls=18.325012]\n",
            "100%|██████████| 1/1 [00:00<00:00, 372.86batch/s, epo=56, lss=0.229508, vls=18.263479]\n",
            "100%|██████████| 1/1 [00:00<00:00, 367.15batch/s, epo=57, lss=0.225457, vls=18.203772]\n",
            "100%|██████████| 1/1 [00:00<00:00, 383.29batch/s, epo=58, lss=0.221536, vls=18.145760]\n",
            "100%|██████████| 1/1 [00:00<00:00, 388.43batch/s, epo=59, lss=0.217741, vls=18.089340]\n",
            "100%|██████████| 1/1 [00:00<00:00, 245.38batch/s, epo=60, lss=0.214070, vls=18.034412]\n",
            "100%|██████████| 1/1 [00:00<00:00, 306.27batch/s, epo=61, lss=0.210517, vls=17.980885]\n",
            "100%|██████████| 1/1 [00:00<00:00, 393.20batch/s, epo=62, lss=0.207078, vls=17.928688]\n",
            "100%|██████████| 1/1 [00:00<00:00, 394.80batch/s, epo=63, lss=0.203749, vls=17.877748]\n",
            "100%|██████████| 1/1 [00:00<00:00, 354.25batch/s, epo=64, lss=0.200524, vls=17.827999]\n",
            "100%|██████████| 1/1 [00:00<00:00, 143.98batch/s, epo=65, lss=0.197400, vls=17.779377]\n",
            "100%|██████████| 1/1 [00:00<00:00, 417.30batch/s, epo=66, lss=0.194371, vls=17.731834]\n",
            "100%|██████████| 1/1 [00:00<00:00, 397.87batch/s, epo=67, lss=0.191435, vls=17.685318]\n",
            "100%|██████████| 1/1 [00:00<00:00, 364.28batch/s, epo=68, lss=0.188586, vls=17.639778]\n",
            "100%|██████████| 1/1 [00:00<00:00, 178.50batch/s, epo=69, lss=0.185821, vls=17.595182]\n",
            "100%|██████████| 1/1 [00:00<00:00, 364.50batch/s, epo=70, lss=0.183137, vls=17.551500]\n",
            "100%|██████████| 1/1 [00:00<00:00, 356.96batch/s, epo=71, lss=0.180531, vls=17.508705]\n",
            "100%|██████████| 1/1 [00:00<00:00, 351.90batch/s, epo=72, lss=0.177999, vls=17.466789]\n",
            "100%|██████████| 1/1 [00:00<00:00, 345.27batch/s, epo=73, lss=0.175538, vls=17.425749]\n",
            "100%|██████████| 1/1 [00:00<00:00, 401.64batch/s, epo=74, lss=0.173146, vls=17.385590]\n",
            "100%|██████████| 1/1 [00:00<00:00, 261.78batch/s, epo=75, lss=0.170819, vls=17.346319]\n",
            "100%|██████████| 1/1 [00:00<00:00, 376.10batch/s, epo=76, lss=0.168555, vls=17.307951]\n",
            "100%|██████████| 1/1 [00:00<00:00, 130.81batch/s, epo=77, lss=0.166351, vls=17.270500]\n",
            "100%|██████████| 1/1 [00:00<00:00, 349.06batch/s, epo=78, lss=0.164205, vls=17.233973]\n",
            "100%|██████████| 1/1 [00:00<00:00, 402.45batch/s, epo=79, lss=0.162113, vls=17.198372]\n",
            "100%|██████████| 1/1 [00:00<00:00, 391.63batch/s, epo=80, lss=0.160074, vls=17.163694]\n",
            "100%|██████████| 1/1 [00:00<00:00, 304.00batch/s, epo=81, lss=0.158084, vls=17.129917]\n",
            "100%|██████████| 1/1 [00:00<00:00, 352.49batch/s, epo=82, lss=0.156143, vls=17.097021]\n",
            "100%|██████████| 1/1 [00:00<00:00, 416.80batch/s, epo=83, lss=0.154247, vls=17.064974]\n",
            "100%|██████████| 1/1 [00:00<00:00, 368.73batch/s, epo=84, lss=0.152394, vls=17.033737]\n",
            "100%|██████████| 1/1 [00:00<00:00, 394.94batch/s, epo=85, lss=0.150584, vls=17.003273]\n",
            "100%|██████████| 1/1 [00:00<00:00, 125.80batch/s, epo=86, lss=0.148813, vls=16.973537]\n",
            "100%|██████████| 1/1 [00:00<00:00, 390.13batch/s, epo=87, lss=0.147080, vls=16.944500]\n",
            "100%|██████████| 1/1 [00:00<00:00, 388.11batch/s, epo=88, lss=0.145384, vls=16.916124]\n",
            "100%|██████████| 1/1 [00:00<00:00, 411.93batch/s, epo=89, lss=0.143723, vls=16.888391]\n",
            "100%|██████████| 1/1 [00:00<00:00, 366.99batch/s, epo=90, lss=0.142096, vls=16.861271]\n",
            "100%|██████████| 1/1 [00:00<00:00, 418.84batch/s, epo=91, lss=0.140501, vls=16.834755]\n",
            "100%|██████████| 1/1 [00:00<00:00, 392.65batch/s, epo=92, lss=0.138938, vls=16.808832]\n",
            "100%|██████████| 1/1 [00:00<00:00, 407.81batch/s, epo=93, lss=0.137404, vls=16.783491]\n",
            "100%|██████████| 1/1 [00:00<00:00, 153.93batch/s, epo=94, lss=0.135899, vls=16.758732]\n",
            "100%|██████████| 1/1 [00:00<00:00, 427.08batch/s, epo=95, lss=0.134422, vls=16.734541]\n",
            "100%|██████████| 1/1 [00:00<00:00, 365.87batch/s, epo=96, lss=0.132972, vls=16.710913]\n",
            "100%|██████████| 1/1 [00:00<00:00, 371.57batch/s, epo=97, lss=0.131547, vls=16.687830]\n",
            "100%|██████████| 1/1 [00:00<00:00, 106.15batch/s, epo=98, lss=0.130148, vls=16.665276]\n",
            "100%|██████████| 1/1 [00:00<00:00, 218.36batch/s, epo=99, lss=0.128773, vls=16.643230]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training stage.\")\n",
        "ae_optimizer = Adam(params=autoencoder.parameters())\n",
        "ae.train(\n",
        "    X_train_data,\n",
        "    autoencoder,\n",
        "    cuda=True,\n",
        "    validation=X_val_data,\n",
        "    epochs=70,\n",
        "    batch_size=50,\n",
        "    optimizer=ae_optimizer,\n",
        "    corruption=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiSxp30Edry1",
        "outputId": "1fabdea5-60e2-4a3f-8d32-1ca7e626432c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training stage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 123.64batch/s, epo=0, lss=0.556736, vls=-1.000000]\n",
            "100%|██████████| 1/1 [00:00<00:00, 126.57batch/s, epo=1, lss=4.658497, vls=35.396729]\n",
            "100%|██████████| 1/1 [00:00<00:00, 185.76batch/s, epo=2, lss=0.772071, vls=30.077034]\n",
            "100%|██████████| 1/1 [00:00<00:00, 142.22batch/s, epo=3, lss=0.804831, vls=30.403105]\n",
            "100%|██████████| 1/1 [00:00<00:00, 158.49batch/s, epo=4, lss=0.887708, vls=30.890818]\n",
            "100%|██████████| 1/1 [00:00<00:00, 118.52batch/s, epo=5, lss=0.783711, vls=30.921640]\n",
            "100%|██████████| 1/1 [00:00<00:00, 152.69batch/s, epo=6, lss=0.727730, vls=30.491247]\n",
            "100%|██████████| 1/1 [00:00<00:00, 132.82batch/s, epo=7, lss=0.645224, vls=29.939440]\n",
            "100%|██████████| 1/1 [00:00<00:00, 174.68batch/s, epo=8, lss=0.606551, vls=29.517780]\n",
            "100%|██████████| 1/1 [00:00<00:00, 182.96batch/s, epo=9, lss=0.565970, vls=29.179085]\n",
            "100%|██████████| 1/1 [00:00<00:00, 141.13batch/s, epo=10, lss=0.515538, vls=28.966202]\n",
            "100%|██████████| 1/1 [00:00<00:00, 185.06batch/s, epo=11, lss=0.482193, vls=28.813604]\n",
            "100%|██████████| 1/1 [00:00<00:00, 151.59batch/s, epo=12, lss=0.463879, vls=28.707645]\n",
            "100%|██████████| 1/1 [00:00<00:00, 135.28batch/s, epo=13, lss=0.458885, vls=28.626133]\n",
            "100%|██████████| 1/1 [00:00<00:00, 162.91batch/s, epo=14, lss=0.432299, vls=28.567434]\n",
            "100%|██████████| 1/1 [00:00<00:00, 170.49batch/s, epo=15, lss=0.404325, vls=28.503160]\n",
            "100%|██████████| 1/1 [00:00<00:00, 159.29batch/s, epo=16, lss=0.384435, vls=28.451511]\n",
            "100%|██████████| 1/1 [00:00<00:00, 154.01batch/s, epo=17, lss=0.376267, vls=28.417801]\n",
            "100%|██████████| 1/1 [00:00<00:00, 150.02batch/s, epo=18, lss=0.350806, vls=28.390076]\n",
            "100%|██████████| 1/1 [00:00<00:00, 175.60batch/s, epo=19, lss=0.334882, vls=28.361340]\n",
            "100%|██████████| 1/1 [00:00<00:00, 162.68batch/s, epo=20, lss=0.322032, vls=28.327633]\n",
            "100%|██████████| 1/1 [00:00<00:00, 170.00batch/s, epo=21, lss=0.310957, vls=28.298250]\n",
            "100%|██████████| 1/1 [00:00<00:00, 145.77batch/s, epo=22, lss=0.301827, vls=28.267071]\n",
            "100%|██████████| 1/1 [00:00<00:00, 139.36batch/s, epo=23, lss=0.287494, vls=28.248030]\n",
            "100%|██████████| 1/1 [00:00<00:00, 163.78batch/s, epo=24, lss=0.277640, vls=28.238344]\n",
            "100%|██████████| 1/1 [00:00<00:00, 136.35batch/s, epo=25, lss=0.280058, vls=28.232994]\n",
            "100%|██████████| 1/1 [00:00<00:00, 142.68batch/s, epo=26, lss=0.263284, vls=28.236061]\n",
            "100%|██████████| 1/1 [00:00<00:00, 160.78batch/s, epo=27, lss=0.280833, vls=28.242968]\n",
            "100%|██████████| 1/1 [00:00<00:00, 145.75batch/s, epo=28, lss=0.258245, vls=28.217274]\n",
            "100%|██████████| 1/1 [00:00<00:00, 167.60batch/s, epo=29, lss=0.246405, vls=28.203680]\n",
            "100%|██████████| 1/1 [00:00<00:00, 161.36batch/s, epo=30, lss=0.234018, vls=28.208328]\n",
            "100%|██████████| 1/1 [00:00<00:00, 139.39batch/s, epo=31, lss=0.230633, vls=28.213331]\n",
            "100%|██████████| 1/1 [00:00<00:00, 189.21batch/s, epo=32, lss=0.223626, vls=28.197222]\n",
            "100%|██████████| 1/1 [00:00<00:00, 140.29batch/s, epo=33, lss=0.216260, vls=28.173105]\n",
            "100%|██████████| 1/1 [00:00<00:00, 134.95batch/s, epo=34, lss=0.230260, vls=28.159065]\n",
            "100%|██████████| 1/1 [00:00<00:00, 132.23batch/s, epo=35, lss=0.202834, vls=28.163242]\n",
            "100%|██████████| 1/1 [00:00<00:00, 174.73batch/s, epo=36, lss=0.226684, vls=28.177523]\n",
            "100%|██████████| 1/1 [00:00<00:00, 142.41batch/s, epo=37, lss=0.199044, vls=28.164923]\n",
            "100%|██████████| 1/1 [00:00<00:00, 162.31batch/s, epo=38, lss=0.196664, vls=28.146183]\n",
            "100%|██████████| 1/1 [00:00<00:00, 132.96batch/s, epo=39, lss=0.196484, vls=28.134747]\n",
            "100%|██████████| 1/1 [00:00<00:00, 173.19batch/s, epo=40, lss=0.204174, vls=28.135620]\n",
            "100%|██████████| 1/1 [00:00<00:00, 146.37batch/s, epo=41, lss=0.178874, vls=28.151417]\n",
            "100%|██████████| 1/1 [00:00<00:00, 166.98batch/s, epo=42, lss=0.187692, vls=28.164551]\n",
            "100%|██████████| 1/1 [00:00<00:00, 148.89batch/s, epo=43, lss=0.183134, vls=28.153080]\n",
            "100%|██████████| 1/1 [00:00<00:00, 145.20batch/s, epo=44, lss=0.167811, vls=28.126841]\n",
            "100%|██████████| 1/1 [00:00<00:00, 130.94batch/s, epo=45, lss=0.172404, vls=28.110857]\n",
            "100%|██████████| 1/1 [00:00<00:00, 158.58batch/s, epo=46, lss=0.178161, vls=28.111673]\n",
            "100%|██████████| 1/1 [00:00<00:00, 156.05batch/s, epo=47, lss=0.157861, vls=28.122871]\n",
            "100%|██████████| 1/1 [00:00<00:00, 165.88batch/s, epo=48, lss=0.158835, vls=28.133343]\n",
            "100%|██████████| 1/1 [00:00<00:00, 142.19batch/s, epo=49, lss=0.158307, vls=28.131495]\n",
            "100%|██████████| 1/1 [00:00<00:00, 155.28batch/s, epo=50, lss=0.145070, vls=28.113863]\n",
            "100%|██████████| 1/1 [00:00<00:00, 131.36batch/s, epo=51, lss=0.142289, vls=28.093039]\n",
            "100%|██████████| 1/1 [00:00<00:00, 159.39batch/s, epo=52, lss=0.135800, vls=28.080538]\n",
            "100%|██████████| 1/1 [00:00<00:00, 160.66batch/s, epo=53, lss=0.139019, vls=28.074369]\n",
            "100%|██████████| 1/1 [00:00<00:00, 169.17batch/s, epo=54, lss=0.129968, vls=28.076153]\n",
            "100%|██████████| 1/1 [00:00<00:00, 182.98batch/s, epo=55, lss=0.129562, vls=28.087299]\n",
            "100%|██████████| 1/1 [00:00<00:00, 174.56batch/s, epo=56, lss=0.126054, vls=28.095909]\n",
            "100%|██████████| 1/1 [00:00<00:00, 122.69batch/s, epo=57, lss=0.123593, vls=28.096132]\n",
            "100%|██████████| 1/1 [00:00<00:00, 117.17batch/s, epo=58, lss=0.117766, vls=28.088495]\n",
            "100%|██████████| 1/1 [00:00<00:00, 149.04batch/s, epo=59, lss=0.114228, vls=28.077381]\n",
            "100%|██████████| 1/1 [00:00<00:00, 134.92batch/s, epo=60, lss=0.112475, vls=28.069494]\n",
            "100%|██████████| 1/1 [00:00<00:00, 179.64batch/s, epo=61, lss=0.108939, vls=28.067087]\n",
            "100%|██████████| 1/1 [00:00<00:00, 192.27batch/s, epo=62, lss=0.105653, vls=28.066242]\n",
            "100%|██████████| 1/1 [00:00<00:00, 177.98batch/s, epo=63, lss=0.105175, vls=28.065815]\n",
            "100%|██████████| 1/1 [00:00<00:00, 163.08batch/s, epo=64, lss=0.103228, vls=28.068760]\n",
            "100%|██████████| 1/1 [00:00<00:00, 190.79batch/s, epo=65, lss=0.100906, vls=28.070704]\n",
            "100%|██████████| 1/1 [00:00<00:00, 173.20batch/s, epo=66, lss=0.094336, vls=28.071648]\n",
            "100%|██████████| 1/1 [00:00<00:00, 180.94batch/s, epo=67, lss=0.095338, vls=28.070616]\n",
            "100%|██████████| 1/1 [00:00<00:00, 169.38batch/s, epo=68, lss=0.089590, vls=28.070698]\n",
            "100%|██████████| 1/1 [00:00<00:00, 153.66batch/s, epo=69, lss=0.090507, vls=28.071568]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SVM classifier stage\")\n",
        "dataloader = DataLoader(X_train_data, batch_size=10, shuffle=False)\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "autoencoder.eval()\n",
        "features = []\n",
        "actual = []\n",
        "print(len(dataloader))\n",
        "for batch in dataloader:\n",
        "    if (isinstance(batch, tuple) or isinstance(batch, list)) and len(batch) == 2:\n",
        "        batch, value = batch  # if we have a prediction label, separate it to actual\n",
        "        actual.append(value)\n",
        "\n",
        "    batch = batch.cuda(non_blocking=True)\n",
        "    features.append(autoencoder.encoder(batch).detach().cpu())\n",
        "\n",
        "print(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHeweO1Hyy_d",
        "outputId": "92025031-2f92-4027-d4ea-fec49df5594e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM classifier stage\n",
            "5\n",
            "[tensor([[-3.3710e+00, -1.0488e+01, -4.9439e+00,  7.3420e+00, -9.4240e-02,\n",
            "          1.1999e+00,  2.6440e+00,  1.5890e+00, -5.0671e+00, -5.8099e+00,\n",
            "          5.4576e+00, -9.8918e+00, -4.7203e+00, -6.2612e+00,  6.5589e+00,\n",
            "         -5.2556e+00,  3.7104e-01, -4.6000e+00, -3.0543e+00, -1.1973e+00,\n",
            "         -9.7804e+00,  3.5161e+00,  2.9512e+00, -2.2396e+00,  2.8980e+00,\n",
            "          5.1707e+00,  2.1721e+00, -2.1138e+00, -3.0351e+00,  1.2062e+00,\n",
            "          4.7342e+00, -3.4866e+00, -2.5070e+00,  1.0398e+01, -1.5839e+00,\n",
            "         -8.0507e-01, -5.5467e+00,  1.1340e+00, -2.6786e+00, -2.3463e+00,\n",
            "          1.3694e+00,  1.5435e+00,  3.4487e+00,  6.8476e+00, -3.4124e+00,\n",
            "         -5.0968e+00, -3.2850e+00, -3.8156e+00,  3.1699e+00, -5.6295e+00],\n",
            "        [-2.9060e-01,  1.2367e+00, -3.8368e-01,  3.3794e+00, -3.2772e+00,\n",
            "         -6.2234e-01,  1.5573e+00, -1.6349e+00,  3.5766e-01,  1.5367e+00,\n",
            "         -8.6714e-01,  5.9580e-01, -1.9662e+00,  2.2905e+00,  4.3145e+00,\n",
            "          3.5722e+00,  1.1586e-01,  2.0532e-01,  4.0057e+00,  3.0353e-01,\n",
            "          1.9497e+00,  1.2813e+00, -1.9304e-01,  1.9671e+00, -6.4455e-01,\n",
            "          2.0785e+00, -3.0420e+00, -2.3377e+00, -1.7093e+00, -7.6375e-01,\n",
            "         -4.3840e+00,  2.2727e+00,  1.9267e-02,  1.3199e+00, -9.2283e-01,\n",
            "         -1.6767e+00, -1.4452e+00,  2.7306e+00, -3.3383e+00, -3.5977e+00,\n",
            "          1.9926e-01, -8.3584e-01,  3.4394e+00, -6.8959e-01, -2.7522e+00,\n",
            "         -3.3726e-02, -3.1955e+00,  1.6365e+00,  1.7613e-01,  1.9144e+00],\n",
            "        [-3.5404e+00, -8.1543e+00, -5.9108e+00,  6.8194e+00, -6.3501e+00,\n",
            "          7.8640e+00, -3.0338e+00, -6.2914e+00, -8.4573e+00, -7.0579e-01,\n",
            "          1.9600e+00, -4.0857e-01,  2.3974e+00,  3.4573e-01,  3.6091e+00,\n",
            "         -2.3337e+00, -9.8223e-01,  3.4926e+00, -4.8263e+00, -1.4788e+00,\n",
            "          3.8180e+00, -1.7264e+00,  8.6259e+00,  3.7914e+00,  1.9261e+00,\n",
            "          1.5597e+00, -4.3180e+00, -2.6130e+00,  7.9340e-01, -5.6881e-01,\n",
            "          3.1504e+00,  3.3770e+00, -5.1545e-01, -2.0877e+00, -5.3406e+00,\n",
            "         -9.4284e-01, -3.2913e+00,  4.3459e+00, -7.9320e-02, -2.1331e-01,\n",
            "         -5.8110e+00,  2.2785e+00,  8.0042e+00,  8.3296e-01, -2.9442e+00,\n",
            "          4.5010e+00,  3.0389e+00, -5.9606e+00, -1.9972e+00,  1.9658e+00],\n",
            "        [-4.8375e+00, -7.4051e+00,  6.4923e+00,  3.1486e+00, -9.8768e+00,\n",
            "          1.0310e+00,  2.6615e+00,  1.9263e-01,  1.8368e+00,  1.8282e-01,\n",
            "         -9.1880e-01, -2.4758e+00,  3.1343e+00, -2.3630e+00,  1.3483e+00,\n",
            "         -2.8517e+00,  4.8033e+00, -3.3896e+00,  2.1278e+00, -2.7835e+00,\n",
            "         -3.6916e-01, -2.6883e+00,  4.2194e-01,  9.1788e-01, -2.9190e+00,\n",
            "         -1.2589e+00, -2.2224e+00, -1.0528e+00, -1.4326e+00,  2.1336e+00,\n",
            "         -3.0511e+00,  3.6738e-02,  5.6866e+00,  9.3839e+00, -3.1257e+00,\n",
            "         -9.2775e+00,  3.3673e-01, -4.7753e+00,  5.6438e+00,  1.4290e+00,\n",
            "          3.4399e+00,  5.0542e+00,  1.2883e+01,  1.9842e+00, -4.1219e+00,\n",
            "         -5.1202e+00,  3.2528e-01, -5.2804e+00, -1.5917e+00, -1.6476e-01],\n",
            "        [-7.1728e-01, -2.5821e-01,  2.5367e+00, -5.5448e-01, -3.6754e+00,\n",
            "          1.7548e+00,  8.7508e-01, -6.8688e-01,  1.2641e+00,  1.6716e+00,\n",
            "         -1.2050e+00,  2.0455e+00,  3.9806e-01,  6.2391e-01,  5.1945e+00,\n",
            "          3.8361e-01,  2.4835e+00, -8.3244e-02,  3.6498e+00, -1.1715e+00,\n",
            "          2.8050e+00,  6.5001e-01, -1.0023e+00, -7.6408e-02, -2.3119e-02,\n",
            "          1.7469e+00, -3.2233e+00, -1.4287e+00, -8.0130e-01,  1.3329e+00,\n",
            "         -5.2826e+00,  2.8334e+00,  2.8410e+00,  5.2675e+00, -2.9748e+00,\n",
            "         -4.0831e+00, -1.3403e+00, -1.2372e+00,  9.0712e-01, -4.7847e+00,\n",
            "          2.5375e+00,  8.7506e-01,  5.2277e+00, -2.6969e-01, -2.0665e+00,\n",
            "          7.9973e-01, -7.7086e-01, -1.3508e+00, -4.3732e-01,  2.7692e+00],\n",
            "        [ 2.4957e-01, -5.4743e+00,  4.2402e-01, -1.9655e+00, -2.0070e+00,\n",
            "          5.5070e-01, -1.8450e+00, -6.2854e+00, -1.0614e+00,  2.0361e-02,\n",
            "          1.8950e+00, -1.4382e+00,  3.2263e+00,  2.4001e-01, -2.3711e+00,\n",
            "          3.7100e+00,  3.5018e+00,  1.6712e+00,  7.2294e-01,  1.9381e+00,\n",
            "          2.3238e+00, -4.0726e+00, -6.4512e-01, -7.6069e+00, -1.1314e+00,\n",
            "          2.2089e+00, -9.6149e-01,  4.4858e+00,  5.8912e+00, -3.9403e-01,\n",
            "         -3.0983e+00,  5.0623e+00,  3.6898e+00, -6.1654e-01, -7.5205e+00,\n",
            "         -1.4968e+00, -2.6425e+00,  1.4680e+00, -1.9915e+00,  4.4427e+00,\n",
            "          6.3979e-01, -1.7310e+00, -1.5673e+00,  8.3805e-01,  8.9957e-01,\n",
            "         -2.4857e+00, -6.8840e+00, -5.4478e+00,  8.2454e-01, -5.8706e+00],\n",
            "        [ 7.3739e+00,  1.2176e+00,  5.4448e+00, -7.4811e-01, -1.9104e+00,\n",
            "          6.5221e+00, -2.0243e+00,  6.4848e+00,  6.4287e+00,  1.2971e+00,\n",
            "          4.7483e+00,  2.5521e+00, -1.2950e+00,  5.7272e+00,  6.0818e+00,\n",
            "          3.8200e+00,  2.6299e-01, -7.2746e+00,  4.3674e+00,  3.0529e+00,\n",
            "          1.3616e+00,  3.5476e+00,  3.4677e+00,  3.5487e+00, -5.5918e+00,\n",
            "          2.5194e+00, -4.0370e+00,  3.7416e-01, -2.0297e+00,  5.9742e-01,\n",
            "         -3.6653e-01, -1.3832e+00,  1.3555e-01,  2.7950e+00, -6.6979e-02,\n",
            "         -3.6356e+00,  2.9808e-01,  1.7131e+00,  4.0224e-01, -2.1974e+00,\n",
            "          7.2598e+00,  4.2219e+00,  4.6059e+00,  3.0108e+00, -3.5988e+00,\n",
            "         -6.7100e+00,  8.6034e-01,  1.0663e+00,  7.0828e+00, -1.6051e-01],\n",
            "        [ 4.7136e+00, -4.1653e-01, -1.2579e+01, -4.6465e+00,  1.4850e+01,\n",
            "         -5.8149e+00, -1.0206e+01, -1.2380e+01, -9.4239e-01,  9.2600e-01,\n",
            "         -1.3401e+01, -6.5764e-01,  1.6420e+01,  3.6552e+00,  8.7254e+00,\n",
            "          2.1315e+00,  1.4041e+01, -5.9252e+00, -2.1398e-01, -9.7052e+00,\n",
            "          2.0913e+00,  1.9409e+01,  1.4925e+01,  3.6124e+00,  9.1469e-02,\n",
            "         -4.1815e+00, -2.5554e+00, -6.6514e+00,  3.9157e+00,  5.3329e+00,\n",
            "          1.0588e+01,  7.1828e+00, -1.2700e+01, -1.5021e-01,  1.9783e+00,\n",
            "          1.6129e-01, -1.6356e+00,  8.9086e+00,  9.7815e-01, -1.0166e+01,\n",
            "         -5.4763e+00,  1.1294e+01, -7.4174e-01,  5.6650e+00, -9.7907e+00,\n",
            "          5.3653e+00, -1.5360e+01, -1.3003e+01,  6.6736e-01, -1.2447e+01],\n",
            "        [-5.9512e-01, -2.9209e-01,  2.4453e+00, -5.1880e-01, -3.2881e+00,\n",
            "          1.7424e+00,  7.5627e-01, -6.7271e-01,  1.1253e+00,  1.5804e+00,\n",
            "         -1.0193e+00,  1.9716e+00,  3.3429e-01,  5.5794e-01,  4.8197e+00,\n",
            "          2.8616e-01,  2.2807e+00, -1.8481e-01,  3.2988e+00, -9.7081e-01,\n",
            "          2.2644e+00,  6.9703e-01, -1.0094e+00, -8.8641e-02, -2.0158e-02,\n",
            "          1.5000e+00, -2.7993e+00, -1.1839e+00, -7.0187e-01,  1.3159e+00,\n",
            "         -4.8268e+00,  2.4258e+00,  2.6535e+00,  4.9462e+00, -2.7248e+00,\n",
            "         -3.8075e+00, -1.1718e+00, -1.1510e+00,  1.0868e+00, -4.3703e+00,\n",
            "          2.3666e+00,  9.3342e-01,  4.8198e+00, -9.4708e-02, -1.8970e+00,\n",
            "          7.7266e-01, -6.3587e-01, -1.2002e+00, -3.1368e-01,  2.5558e+00],\n",
            "        [-4.6644e+00, -4.3295e+00,  2.9274e+00, -1.6733e+00, -7.0020e+00,\n",
            "          4.4835e+00,  1.2928e+00, -2.7496e+00,  1.4753e-01, -2.3514e+00,\n",
            "          1.7165e+00, -1.7939e+00, -8.3450e-01, -3.5347e+00,  8.4043e-01,\n",
            "         -1.6745e+00, -3.2919e-01, -7.4054e+00,  4.0002e+00, -1.5261e+00,\n",
            "         -1.9736e+00,  1.7788e+00,  1.5899e+00,  2.0543e-01,  3.2607e+00,\n",
            "         -2.6488e+00, -6.3528e-01, -4.7574e+00,  2.5207e+00,  3.6788e+00,\n",
            "         -1.5687e+00, -5.2431e-01,  1.7984e+00,  1.2906e+01, -4.8367e+00,\n",
            "         -6.4275e+00, -1.7318e+00, -6.9775e+00,  4.2853e+00, -2.9891e+00,\n",
            "          2.7144e+00, -6.1433e-01,  2.7307e+00,  4.7361e+00, -6.8027e+00,\n",
            "          7.0518e-01, -2.8501e+00, -4.9351e+00, -9.1038e-01, -3.6678e+00]]), tensor([[-2.7652e-01,  8.6453e-02,  1.6232e+00, -9.3369e-01, -1.7022e+00,\n",
            "          9.8725e-01,  1.3964e-01, -4.7495e-01,  4.3702e-01,  7.6669e-01,\n",
            "         -7.2504e-01,  1.5078e+00,  7.0899e-01,  1.3429e-01,  3.4193e+00,\n",
            "         -9.8547e-02,  1.8434e+00,  1.4150e-02,  2.3584e+00, -6.2465e-01,\n",
            "          1.3548e+00,  3.7595e-01, -4.8248e-01, -4.9548e-01,  1.7744e-01,\n",
            "          1.0202e+00, -2.0580e+00, -1.0858e+00, -1.6916e-01,  1.0941e+00,\n",
            "         -3.2067e+00,  1.9961e+00,  2.1332e+00,  3.3530e+00, -1.9615e+00,\n",
            "         -2.4037e+00, -6.8271e-01, -1.0243e+00,  1.2569e+00, -3.2744e+00,\n",
            "          1.4802e+00,  4.2696e-01,  3.1909e+00,  8.6231e-02, -6.6117e-01,\n",
            "          1.0854e+00, -5.5420e-01, -7.3396e-01, -3.1129e-03,  1.9341e+00],\n",
            "        [-1.0301e-01,  5.1078e-01,  5.4305e-03,  1.9375e+00, -2.9758e+00,\n",
            "          1.4362e+00,  8.2352e-03, -1.0707e+00,  9.4447e-01,  1.3203e+00,\n",
            "          7.9533e-01, -2.5927e-01, -7.0329e-02,  1.2176e+00,  5.4273e+00,\n",
            "          1.4044e+00,  8.8706e-01, -1.9519e+00,  3.4791e+00,  9.8650e-02,\n",
            "          2.0580e+00,  9.4342e-01, -8.7758e-01,  5.4026e-01,  7.5405e-01,\n",
            "          1.0524e+00, -2.9662e+00, -1.0063e+00, -1.2635e-01,  1.5399e+00,\n",
            "         -2.5096e+00,  3.0102e+00,  3.0509e+00,  4.5800e+00, -1.8427e+00,\n",
            "         -3.0027e+00,  7.5899e-01,  3.6745e-02,  1.0418e+00, -3.6490e+00,\n",
            "         -8.2059e-01,  3.6542e-02,  4.2965e+00, -5.4577e-01, -1.5469e+00,\n",
            "          6.8714e-01, -1.3002e+00,  9.9345e-01,  7.1801e-01,  1.5987e+00],\n",
            "        [-1.8593e+00,  1.1865e+00,  2.0152e+00,  2.9092e-01, -2.6913e+00,\n",
            "          2.3920e+00, -1.0943e+00, -9.3931e-01,  1.1066e+00,  3.3348e+00,\n",
            "          1.4811e-01,  1.8135e-01,  8.7118e-01,  3.2611e+00,  5.8040e+00,\n",
            "          1.1259e+00,  2.1483e+00, -2.4776e+00,  3.4402e+00,  2.0986e+00,\n",
            "          4.7161e+00,  1.7824e+00, -2.2312e+00, -4.1272e-01, -2.8625e-01,\n",
            "          2.9810e-01, -4.7435e+00, -3.2196e-01, -1.7414e-01,  7.6019e-01,\n",
            "         -4.7685e+00,  3.3827e+00,  2.6073e+00,  4.5903e+00, -1.5295e+00,\n",
            "         -4.0372e+00, -1.0266e+00, -2.4608e+00,  1.7373e+00, -5.3596e+00,\n",
            "          4.1914e-01,  3.7312e-01,  4.6032e+00, -1.8357e+00, -2.1403e+00,\n",
            "          1.9344e+00, -5.7080e-01, -2.3327e+00,  1.2594e+00,  4.2461e+00],\n",
            "        [ 1.4803e+00, -2.6562e+00,  2.9531e+00, -4.8137e+00, -2.0165e-01,\n",
            "         -3.5782e+00,  2.9812e+00, -7.6874e-01,  6.6984e-01, -1.5784e-01,\n",
            "         -2.9438e+00,  5.1784e+00,  2.4885e+00,  1.2555e+00,  7.0453e-01,\n",
            "          6.9876e+00,  8.3959e+00,  2.8767e+00, -7.3798e-01,  1.9286e+00,\n",
            "         -3.1436e+00,  7.1815e-01,  1.8605e+00, -2.7621e+00, -4.3315e+00,\n",
            "          4.6896e+00, -1.8858e-01,  1.1851e+00,  6.2185e-01, -2.2438e+00,\n",
            "         -5.8210e+00,  5.2194e+00, -2.9698e+00,  1.5985e+00, -3.8442e+00,\n",
            "         -2.0124e+00, -4.9794e+00,  2.6411e+00, -1.5249e+00, -3.2355e+00,\n",
            "          5.1681e+00, -3.9925e-01,  7.9712e+00,  5.5756e+00,  1.0356e+00,\n",
            "         -1.8347e+00, -7.4248e+00, -4.7692e+00,  4.7659e+00, -8.7388e-01],\n",
            "        [-5.8672e-01, -9.6158e-02,  2.1188e+00, -4.4390e-01, -2.7811e+00,\n",
            "          1.6072e+00,  4.6122e-01, -8.0960e-01,  8.7070e-01,  1.8294e+00,\n",
            "         -7.5378e-01,  1.7834e+00,  2.0094e-01,  6.8394e-01,  4.1884e+00,\n",
            "          4.3869e-01,  1.9135e+00, -1.5053e-02,  3.0426e+00, -6.5648e-01,\n",
            "          2.2334e+00,  7.1089e-01, -1.1636e+00, -2.4798e-01, -3.3027e-01,\n",
            "          1.2984e+00, -2.6595e+00, -1.0117e+00, -6.3809e-01,  9.2481e-01,\n",
            "         -4.5724e+00,  2.2073e+00,  2.2840e+00,  4.0935e+00, -2.5797e+00,\n",
            "         -3.0769e+00, -1.2067e+00, -8.6570e-01,  8.0219e-01, -3.8203e+00,\n",
            "          2.0970e+00,  5.9651e-01,  4.0310e+00, -2.2578e-01, -1.5991e+00,\n",
            "          8.5937e-01, -5.2842e-01, -1.1960e+00, -3.7781e-01,  2.4446e+00],\n",
            "        [-7.5995e-01, -6.4372e-01,  2.2131e+00,  2.0962e-03, -3.8667e+00,\n",
            "          1.7270e+00,  9.9786e-01, -1.3361e+00,  1.1595e+00,  1.1886e+00,\n",
            "         -7.3443e-01,  1.6271e+00, -1.4873e-01,  2.1179e-01,  5.2292e+00,\n",
            "          4.0131e-01,  2.2098e+00, -2.5914e-01,  3.8894e+00, -1.0513e+00,\n",
            "          2.0085e+00,  3.7842e-01, -9.3206e-01,  8.3506e-02,  8.9796e-02,\n",
            "          1.5796e+00, -3.0270e+00, -1.5439e+00, -5.2951e-01,  1.5734e+00,\n",
            "         -5.3334e+00,  2.6423e+00,  3.0668e+00,  5.2253e+00, -3.4305e+00,\n",
            "         -3.8551e+00, -1.3151e+00, -8.8957e-01,  1.0492e+00, -4.3078e+00,\n",
            "          2.2273e+00,  7.4673e-01,  4.9834e+00, -4.2872e-02, -2.4657e+00,\n",
            "          1.0189e+00, -8.2208e-01, -1.1230e+00, -1.6272e-01,  2.0635e+00],\n",
            "        [ 2.4983e-01, -5.6486e+00,  1.7081e+00,  2.4281e+00, -5.2594e+00,\n",
            "          4.1154e+00,  2.4048e+00, -1.0670e+00, -3.0759e+00,  7.5937e-01,\n",
            "          3.2651e-01,  4.8939e+00, -3.5178e+00,  1.6189e+00,  1.2861e+01,\n",
            "         -4.7112e+00,  1.7848e+00,  1.0702e+00, -1.1010e+00,  1.3429e+00,\n",
            "          3.8021e-01,  7.1589e+00,  2.5991e-01,  3.4503e-02, -1.9320e+00,\n",
            "         -1.6248e+00,  3.2196e+00, -1.0080e+00,  5.7640e+00,  1.2961e+00,\n",
            "         -5.7485e+00, -6.0086e+00,  7.8170e+00,  3.8947e+00, -1.1695e+00,\n",
            "         -6.2627e+00, -4.9182e+00, -1.6778e+00,  5.9926e+00, -6.2531e-01,\n",
            "         -8.1814e-02,  6.2718e+00,  2.2409e+00,  3.0021e+00, -3.9267e+00,\n",
            "         -3.7052e+00,  3.0244e+00, -3.7306e+00,  1.2845e+00, -3.4992e+00],\n",
            "        [ 1.2906e+00, -2.7157e-01, -3.1371e+00,  6.2041e+00, -2.1479e+00,\n",
            "          2.2210e+00, -4.9651e+00,  2.5140e+00, -1.6824e+00, -2.4016e-01,\n",
            "          4.7053e+00, -1.6052e+00, -4.2059e+00,  1.6762e+00,  4.3623e+00,\n",
            "         -5.4716e+00, -2.3002e-01, -1.4495e+00,  9.5028e-01,  5.2371e-01,\n",
            "         -3.5627e+00,  2.4009e+00,  1.9288e+00,  8.0825e-01, -5.3181e-01,\n",
            "          1.0340e+00, -1.3617e-01, -3.4554e+00,  1.1962e+00,  1.5622e+00,\n",
            "          2.2923e+00, -6.4556e+00, -4.1056e+00,  3.1064e+00, -5.7685e-01,\n",
            "         -6.4622e-01,  5.1695e+00,  6.6119e-01,  6.2285e-03,  7.3793e+00,\n",
            "         -4.8911e+00, -5.0059e+00, -1.1244e+00, -1.5224e+00,  4.3251e+00,\n",
            "         -4.4675e+00,  7.1506e+00,  5.2627e+00,  3.6716e-01,  4.2761e+00],\n",
            "        [-6.5115e-01, -3.0208e-01,  2.5298e+00, -4.7201e-01, -3.6087e+00,\n",
            "          1.8509e+00,  8.3378e-01, -7.3701e-01,  1.2111e+00,  1.7308e+00,\n",
            "         -1.0673e+00,  2.0569e+00,  3.5083e-01,  5.8930e-01,  5.2557e+00,\n",
            "          3.7809e-01,  2.4494e+00, -1.6692e-01,  3.6039e+00, -1.1037e+00,\n",
            "          2.5668e+00,  7.0625e-01, -1.0970e+00, -8.7643e-02, -1.8622e-02,\n",
            "          1.6714e+00, -3.0866e+00, -1.3174e+00, -7.6100e-01,  1.4264e+00,\n",
            "         -5.2115e+00,  2.7380e+00,  2.9099e+00,  5.3568e+00, -2.9850e+00,\n",
            "         -4.0892e+00, -1.2316e+00, -1.1885e+00,  1.0875e+00, -4.7246e+00,\n",
            "          2.4672e+00,  9.4158e-01,  5.2505e+00, -1.6090e-01, -2.0230e+00,\n",
            "          8.1337e-01, -7.2765e-01, -1.2540e+00, -3.8205e-01,  2.7273e+00],\n",
            "        [-1.3440e+00, -4.9329e+00, -9.3365e-01,  3.2902e+00,  2.2903e+00,\n",
            "         -2.1463e+00, -3.1247e+00,  6.5635e+00,  1.7198e+00,  7.1915e+00,\n",
            "         -1.6370e+00, -2.2205e+00, -6.0690e-01, -1.7134e+00,  1.9335e+00,\n",
            "         -1.1324e+00,  2.1280e-01, -3.0234e+00, -1.3680e+01,  4.2090e-01,\n",
            "          2.5755e+00, -1.1749e+00, -4.3113e+00, -4.5528e+00, -4.9630e+00,\n",
            "         -5.0601e+00, -1.6260e+00,  9.7258e+00, -9.7820e+00, -3.6542e+00,\n",
            "         -1.9907e+00, -3.2504e-01, -9.0417e+00,  4.8116e+00,  4.9281e+00,\n",
            "          2.1036e+00, -1.6110e+00, -9.7281e+00, -4.6226e+00, -5.5500e+00,\n",
            "         -7.2894e+00,  2.0982e+00,  3.8850e+00, -5.3227e+00, -2.2199e+00,\n",
            "          6.9254e-01,  3.0301e+00, -7.6585e+00,  1.4272e+00, -4.0062e+00]]), tensor([[ 2.4747e-01,  2.3157e+00,  1.0876e+00, -1.0377e+00,  1.2875e+01,\n",
            "          1.7465e+00, -4.0307e+00,  1.5034e+00, -5.7475e+00,  1.3008e+01,\n",
            "          4.5499e+00, -1.0299e+00, -8.0668e+00, -7.2162e+00,  3.9784e+00,\n",
            "         -1.8300e+01, -1.3636e+01, -1.0559e+00, -8.3415e+00, -7.5831e+00,\n",
            "          2.5490e-01,  9.0812e+00, -8.6223e+00, -4.8560e+00, -1.2874e+01,\n",
            "         -2.9522e-01, -1.5099e+01, -5.8653e+00, -9.8574e-01,  4.5175e+00,\n",
            "         -3.5811e+00,  2.5473e+00, -5.4906e+00,  4.4144e+00,  1.1757e+01,\n",
            "          1.5033e+01, -1.5839e+01,  3.2097e+00,  2.2032e+00, -5.5038e+00,\n",
            "         -5.4196e+00, -1.1913e+00,  1.2635e+01, -6.1540e-01,  5.2537e+00,\n",
            "         -1.4494e+00,  1.7537e+00, -1.0019e+01, -2.3525e+00, -3.0400e+00],\n",
            "        [-2.9735e+00, -1.6853e-01, -2.1615e+00,  1.9951e+00, -1.8538e-01,\n",
            "          1.9454e+00,  3.9614e+00,  1.7779e+00,  2.1931e+00,  7.2545e-01,\n",
            "         -1.9877e+00, -3.0196e+00,  2.9922e+00,  2.1374e+00, -9.0288e-01,\n",
            "         -5.4798e+00, -2.0576e-01, -9.0781e-01, -3.3894e-01,  3.8739e+00,\n",
            "          2.2662e+00,  5.4299e-01, -1.7754e-01,  2.3830e-01, -2.6529e+00,\n",
            "         -4.3665e+00,  1.9487e+00,  2.1462e+00, -2.0185e+00,  5.6335e-01,\n",
            "         -5.3940e+00,  1.8853e+00, -1.4039e+00,  1.7151e+00, -9.7064e-01,\n",
            "         -2.3479e+00, -1.0747e+00, -2.3781e+00, -5.5857e-01,  3.7108e-03,\n",
            "         -2.1574e+00, -8.8401e-01,  2.2778e+00, -1.1176e-01,  7.0208e-01,\n",
            "         -2.2298e+00, -1.7677e+00, -2.8183e+00, -2.8510e+00,  2.4224e-01],\n",
            "        [ 1.5543e+00, -2.5628e+00, -5.6759e+00,  1.9558e+00, -2.3021e-01,\n",
            "          2.4096e+00,  1.0523e+00,  5.0142e+00, -3.7075e+00,  2.7011e+00,\n",
            "         -9.7151e-01, -1.7068e-01,  3.2630e-01, -5.6988e+00,  1.3276e+00,\n",
            "         -2.1887e+00,  4.3681e+00,  8.8854e-01, -9.4838e-01, -6.2295e+00,\n",
            "         -4.0239e+00,  1.3351e-01, -3.6125e+00, -1.0583e+00,  5.3000e+00,\n",
            "         -3.1164e-01, -6.1491e+00,  1.7847e+00, -8.1433e+00, -5.7803e+00,\n",
            "          8.5285e-01, -2.6276e+00,  3.7408e+00,  6.3567e+00,  1.0230e+01,\n",
            "          6.0200e+00, -2.8307e+00, -2.9908e+00,  3.6902e+00,  2.3997e+00,\n",
            "         -4.5156e-01, -1.4450e+00, -2.7430e+00, -7.0360e-01, -5.9800e+00,\n",
            "         -3.4728e+00, -4.6200e-02,  4.5046e+00, -1.2284e+00,  2.3680e+00],\n",
            "        [-6.4682e+00,  2.1488e+00, -1.2217e+00,  1.1311e+00, -1.7861e+00,\n",
            "          2.1478e+00, -1.1993e+00,  7.3524e-02,  1.3778e+00,  6.8758e+00,\n",
            "          1.0555e+00, -4.9866e+00,  3.5915e-02,  7.6185e+00,  3.2598e+00,\n",
            "          2.3435e+00,  5.9629e-01, -3.2061e+00, -1.9764e+00,  7.2451e+00,\n",
            "          4.0622e+00, -1.4041e+00, -5.0238e+00, -9.1789e-01, -5.2600e+00,\n",
            "         -2.9805e+00, -5.7363e+00,  4.8751e-01,  3.8650e+00, -1.0831e-01,\n",
            "         -4.2044e+00, -2.6791e-01,  3.0008e+00, -3.1500e-01,  3.9332e-01,\n",
            "         -2.1187e+00,  2.1485e-01, -1.5330e+00,  3.5272e+00, -1.1995e+00,\n",
            "          8.7145e-01,  2.7930e+00, -1.2874e+00, -5.2937e+00, -3.1204e+00,\n",
            "          1.7239e+00,  3.4706e+00, -4.4204e+00,  1.7512e+00,  2.5532e+00],\n",
            "        [-3.9788e+00,  1.2200e-01, -3.3540e+00, -5.3321e+00, -2.2853e+00,\n",
            "         -1.7839e-01, -5.0536e+00,  1.6898e+00,  2.5769e+00,  5.5953e+00,\n",
            "          2.3672e+00, -4.7819e+00,  2.3970e-01, -5.1431e-01, -8.6930e-01,\n",
            "          3.9458e+00, -4.3597e+00, -5.3310e+00,  8.5767e-01,  1.3035e-01,\n",
            "          9.4510e+00, -2.5749e+00,  4.1633e+00, -5.8513e-01, -1.5626e+00,\n",
            "          6.0235e+00,  2.3798e+00, -4.5711e+00,  7.0596e-01, -2.8734e+00,\n",
            "         -1.6772e-01,  4.8956e-01,  7.2593e-02,  3.3409e+00,  1.2452e+00,\n",
            "         -4.6898e-01,  4.1526e+00,  1.1042e+00,  5.4965e+00, -4.4920e+00,\n",
            "         -2.9217e+00, -4.4199e-01, -4.9428e-01, -2.2669e+00, -9.0797e+00,\n",
            "         -3.7255e+00,  4.6771e+00,  3.2273e+00,  3.5305e+00,  2.3807e+00],\n",
            "        [ 4.3139e-02,  3.0535e-01,  3.9919e-01,  2.9585e-01, -1.6323e+00,\n",
            "          5.7677e-01,  8.5608e-02, -6.7823e-01,  3.7633e-01,  9.9853e-01,\n",
            "         -1.3674e-01,  7.9423e-01,  6.3201e-02,  3.6048e-01,  3.0000e+00,\n",
            "          6.9989e-01,  8.6532e-01,  6.3575e-02,  2.3947e+00, -5.7895e-01,\n",
            "          1.6007e+00,  5.1152e-01, -5.5604e-01, -1.8648e-01, -1.7528e-02,\n",
            "          1.1254e+00, -1.9556e+00, -1.0254e+00, -3.8269e-01,  5.2218e-01,\n",
            "         -2.4596e+00,  2.0210e+00,  1.7067e+00,  2.4431e+00, -1.6922e+00,\n",
            "         -1.4180e+00, -2.5687e-01,  1.8896e-01,  1.3912e-01, -2.5005e+00,\n",
            "          5.7938e-01, -3.2104e-01,  2.5120e+00, -2.6646e-01, -5.3049e-01,\n",
            "          6.7949e-01, -8.3004e-01,  1.9922e-01, -3.6925e-01,  1.4011e+00],\n",
            "        [ 1.2372e-03,  2.3767e-01,  2.6568e+00, -1.0440e+00, -2.8198e+00,\n",
            "          1.9575e+00,  3.3751e-01, -6.7637e-01,  1.0148e+00,  1.9092e+00,\n",
            "         -9.6634e-01,  2.6864e+00,  4.2739e-01,  7.7588e-01,  5.2729e+00,\n",
            "          3.2586e-01,  2.3559e+00, -1.6074e-01,  3.9698e+00, -8.1763e-01,\n",
            "          2.2383e+00,  1.0909e+00, -1.0490e+00, -3.6686e-01, -3.8696e-01,\n",
            "          1.7419e+00, -3.2401e+00, -1.5858e+00, -6.9918e-01,  1.1445e+00,\n",
            "         -5.1927e+00,  2.6391e+00,  2.8919e+00,  4.8742e+00, -3.0462e+00,\n",
            "         -3.4455e+00, -1.2172e+00, -8.0425e-01,  1.3478e+00, -4.8599e+00,\n",
            "          2.7673e+00,  7.4987e-01,  4.9271e+00,  1.1823e-01, -1.5157e+00,\n",
            "          1.1896e+00, -8.2945e-01, -8.6375e-01, -3.4495e-01,  2.9777e+00],\n",
            "        [-6.5652e-01, -4.8975e-01, -2.3377e+00,  5.4090e+00, -1.9975e+00,\n",
            "          4.8836e-01, -2.3693e+00, -3.3390e+00, -1.0918e+00, -1.5667e+00,\n",
            "          5.5913e+00, -2.4729e+00, -6.4208e+00, -8.3182e-01,  1.9370e+00,\n",
            "          2.2485e+00, -1.5064e+00,  1.0501e-02,  3.6305e+00,  3.4895e+00,\n",
            "          1.6581e+00, -5.9302e-01, -1.3716e+00,  7.3212e-01, -2.8983e+00,\n",
            "         -1.4184e+00, -4.3535e+00, -1.5642e+00,  6.9586e-01,  4.9202e-01,\n",
            "         -5.0777e+00, -8.2732e-01,  3.2594e-01, -3.8762e+00, -4.5168e+00,\n",
            "          4.3831e+00, -3.2037e+00,  2.0129e+00, -1.2792e+00,  2.6845e+00,\n",
            "         -4.6426e-01, -2.5280e+00, -4.0662e+00, -9.9717e-02, -3.9646e+00,\n",
            "          3.3433e+00,  1.9146e+00, -4.5149e-01,  1.4126e+00, -1.5347e+00],\n",
            "        [-5.5088e+00, -2.4661e+00, -6.0071e+00,  9.2350e-01, -5.0909e+00,\n",
            "         -4.3445e+00,  4.8542e+00, -3.9434e-01,  9.7552e-01,  2.9981e+00,\n",
            "          1.8296e+00,  1.8763e+00, -4.3034e+00, -5.1052e+00,  4.6884e+00,\n",
            "         -4.1330e+00,  2.5822e+00, -1.9325e+00,  4.0243e+00,  1.7889e+00,\n",
            "         -1.0804e+00,  5.6254e+00,  3.5786e+00,  1.8376e+00, -9.4239e-01,\n",
            "          2.8476e-01, -2.5080e+00, -1.9058e+00, -5.4362e-01,  1.4713e+00,\n",
            "          1.9384e+00,  1.5542e+00,  2.7793e+00, -1.3668e+00,  4.7550e-01,\n",
            "         -3.5779e-01,  2.6200e+00, -6.6874e+00,  7.7689e+00,  1.6860e+00,\n",
            "         -9.6916e+00,  2.6316e+00, -1.9855e+00, -3.3251e+00, -1.5824e+00,\n",
            "         -6.0729e-01, -1.5232e+00, -5.8509e+00,  1.7806e+00,  3.7015e+00],\n",
            "        [-8.2266e-01, -5.4320e-01,  3.9758e+00, -1.8936e+00, -3.7512e+00,\n",
            "          1.6654e+00,  1.3385e+00,  1.3599e-01,  1.7358e+00,  1.3883e+00,\n",
            "         -2.5579e+00,  2.9478e+00,  1.1955e+00,  4.7099e-01,  4.6260e+00,\n",
            "         -2.9820e-01,  3.1330e+00,  1.2876e-01,  2.7886e+00, -1.9766e+00,\n",
            "          2.2923e+00,  8.0448e-01, -4.4158e-01, -1.3469e-01,  4.0347e-01,\n",
            "          2.0881e+00, -2.3105e+00, -1.1343e+00, -1.2293e+00,  1.3224e+00,\n",
            "         -5.2000e+00,  2.3538e+00,  2.2979e+00,  5.9646e+00, -2.2622e+00,\n",
            "         -5.1927e+00, -1.5914e+00, -1.9495e+00,  1.2562e+00, -5.3164e+00,\n",
            "          3.7209e+00,  1.7416e+00,  5.7141e+00,  1.0922e-01, -2.1253e+00,\n",
            "          1.0295e-01, -7.1806e-01, -1.9111e+00, -7.6072e-01,  3.3643e+00]]), tensor([[-3.3716e-01,  1.1568e+00,  1.5698e+00, -1.1823e+00, -2.3665e+00,\n",
            "          1.4194e+00, -8.8163e-01, -9.6359e-01,  9.5769e-01,  1.9417e+00,\n",
            "         -6.6857e-01,  2.5564e-01,  1.5587e+00,  7.6832e-01,  2.8833e+00,\n",
            "          1.1667e+00,  1.2800e+00, -8.0492e-01,  1.5230e+00, -3.5899e-01,\n",
            "          1.0747e+00, -1.9884e-01, -2.9957e-01, -6.3980e-01,  2.7623e-02,\n",
            "          1.9052e+00, -2.0210e+00, -1.0605e+00,  1.9620e-01,  5.8070e-01,\n",
            "         -2.9896e+00,  3.1276e+00,  2.7406e+00,  3.8737e+00, -2.0237e+00,\n",
            "         -2.1241e+00, -5.1352e-01, -2.2951e-02,  1.2560e+00, -3.0221e+00,\n",
            "         -7.5965e-01, -4.7696e-01,  2.9272e+00, -4.4231e-01, -5.0278e-01,\n",
            "         -1.5578e-01, -8.8627e-01, -9.5690e-01,  3.7576e-01,  1.2603e+00],\n",
            "        [-7.7566e-01, -1.3501e+00, -1.3770e-01, -2.8166e+00,  6.4027e-01,\n",
            "          1.9018e+00,  1.2095e+00,  1.2309e+00,  1.7210e+00,  2.0636e+00,\n",
            "          9.9285e-01,  4.3582e+00,  1.7741e+00,  4.9939e+00,  6.5386e+00,\n",
            "         -2.3241e+00, -2.6481e+00,  6.3788e+00, -4.6207e+00,  2.2700e+00,\n",
            "          3.3701e+00, -5.6954e+00,  1.2211e+00,  1.8576e+00, -2.9351e+00,\n",
            "         -1.1147e+00,  1.9726e+00,  1.3139e+00, -2.5444e+00,  4.8941e+00,\n",
            "         -5.5648e+00,  4.5258e+00,  1.2192e+00,  2.3760e+00,  3.2877e+00,\n",
            "          1.7853e+00,  4.7343e+00, -1.7717e+00,  4.6039e+00, -2.3074e+00,\n",
            "          5.2471e+00,  4.2562e+00,  1.0741e+00, -1.2472e+00, -2.5642e+00,\n",
            "          2.3315e+00,  1.0364e+01,  2.8137e+00, -1.4895e+00,  1.7421e+00],\n",
            "        [-8.2468e-01, -4.3296e-01,  2.5066e+00, -3.1665e-01, -3.5280e+00,\n",
            "          1.8358e+00,  7.9086e-01, -8.5553e-01,  1.2146e+00,  1.3017e+00,\n",
            "         -8.9731e-01,  1.7001e+00,  2.6210e-01,  6.0899e-01,  5.1157e+00,\n",
            "          1.7001e-01,  2.3305e+00, -5.9861e-01,  3.4329e+00, -7.0587e-01,\n",
            "          2.1234e+00,  6.7585e-01, -9.9215e-01,  7.0826e-02,  2.2703e-01,\n",
            "          1.2740e+00, -2.9131e+00, -1.1752e+00, -5.4562e-01,  1.5854e+00,\n",
            "         -4.9039e+00,  2.3756e+00,  2.8367e+00,  5.2298e+00, -2.7422e+00,\n",
            "         -4.1544e+00, -1.1583e+00, -1.4554e+00,  1.4016e+00, -4.4716e+00,\n",
            "          2.2140e+00,  1.0993e+00,  4.9553e+00, -7.5071e-02, -2.2927e+00,\n",
            "          9.4757e-01, -6.1935e-01, -1.3064e+00,  7.8909e-02,  2.4798e+00],\n",
            "        [-5.4186e+00, -4.2834e+00,  9.2708e-01, -7.8834e-02, -1.1650e+00,\n",
            "         -5.4431e+00,  3.6918e+00, -2.2487e+00,  1.4412e-01, -5.2193e-01,\n",
            "         -2.4126e+00, -1.6533e+00, -7.7431e-01, -4.8245e+00,  5.0330e+00,\n",
            "         -1.2572e+00,  7.5009e+00, -1.6247e+00,  6.8476e-01, -2.4512e+00,\n",
            "         -2.4685e+00,  1.1759e+00,  3.6938e+00, -3.3733e+00, -1.6247e+00,\n",
            "          5.4152e+00,  3.4380e+00, -2.9212e+00,  2.3624e+00, -1.7514e+00,\n",
            "         -1.6535e+00,  5.6277e+00,  1.2725e+00,  4.4982e+00, -8.1128e-01,\n",
            "         -5.3660e+00, -1.2119e+00,  1.0218e+00,  4.1583e+00, -5.9114e+00,\n",
            "          1.6567e+00, -1.5021e+00,  3.4412e+00, -3.4710e+00, -4.2678e+00,\n",
            "          1.4636e+00, -3.0880e+00, -5.9985e+00, -5.3450e-01,  1.3937e+00],\n",
            "        [-5.7685e-01, -1.7653e-01,  2.2387e+00, -5.6258e-01, -3.2566e+00,\n",
            "          1.7045e+00,  7.8996e-01, -4.4481e-01,  1.1274e+00,  1.5742e+00,\n",
            "         -1.0565e+00,  1.8411e+00,  3.8598e-01,  5.7596e-01,  4.5536e+00,\n",
            "          3.7070e-01,  2.2650e+00, -7.1936e-02,  3.1703e+00, -1.0181e+00,\n",
            "          2.6877e+00,  4.5273e-01, -8.8630e-01, -3.8996e-02, -1.7408e-01,\n",
            "          1.5519e+00, -2.9912e+00, -1.2907e+00, -6.7331e-01,  1.1646e+00,\n",
            "         -4.6400e+00,  2.5269e+00,  2.5363e+00,  4.5989e+00, -2.6786e+00,\n",
            "         -3.5528e+00, -1.2116e+00, -1.1675e+00,  7.4753e-01, -4.1592e+00,\n",
            "          2.2788e+00,  8.3617e-01,  4.6509e+00, -3.1025e-01, -1.8014e+00,\n",
            "          6.1107e-01, -5.5631e-01, -1.2904e+00, -3.6959e-01,  2.3871e+00],\n",
            "        [ 1.0698e+00, -1.1225e+00,  1.3806e+00,  1.7691e+00, -3.9213e+00,\n",
            "          1.0120e+01,  5.5338e-01,  2.2996e+00,  6.9970e-01,  2.3206e+00,\n",
            "          4.9815e+00, -9.5150e-01,  6.4774e-01, -1.2632e-01,  9.1053e+00,\n",
            "         -2.4871e-01,  4.5915e+00, -7.7090e+00,  6.9473e-01,  3.3111e+00,\n",
            "          5.5860e-01, -3.2588e+00, -2.4790e+00,  2.2749e+00, -8.5434e-01,\n",
            "         -2.3698e+00, -4.3120e+00,  8.9105e-01,  4.2268e+00,  7.9227e+00,\n",
            "         -1.7677e-01,  1.5200e+00,  8.6215e+00,  9.7843e+00, -4.2986e+00,\n",
            "         -6.2326e+00,  3.2491e+00, -5.6742e+00,  8.8705e+00, -1.3202e+00,\n",
            "         -2.4583e+00,  6.8470e+00,  9.0098e+00,  9.6587e-01, -2.4931e+00,\n",
            "         -7.8051e-01,  4.2757e+00, -1.4799e+00,  6.5802e+00, -2.7843e+00],\n",
            "        [ 2.6395e-01,  3.4860e+00, -1.6808e+01, -5.7806e-01,  2.9473e+00,\n",
            "          2.6477e+00, -9.2110e+00, -2.3172e+00, -4.6885e+00, -4.1528e+00,\n",
            "          1.4001e+01, -1.0131e+01,  4.2463e-01,  1.6413e+00,  8.6072e+00,\n",
            "         -7.5701e+00,  1.0461e+00,  8.0289e+00, -4.8464e+00,  3.1849e+00,\n",
            "         -3.6733e+00,  1.1345e+00,  5.2720e+00, -6.4428e+00, -1.0243e+01,\n",
            "         -2.5065e+00, -9.9363e+00, -2.0523e+00,  4.5091e+00,  2.4284e+00,\n",
            "          4.0502e+00,  6.6470e+00, -4.7381e+00,  1.5116e+00,  1.6717e+00,\n",
            "          7.5370e+00, -3.1049e+00, -1.0453e+01, -2.6657e+00, -7.7255e+00,\n",
            "         -8.1438e+00,  1.3096e+00, -2.8854e-01,  6.4966e+00,  6.6255e-01,\n",
            "          4.8499e-01,  1.4051e+00, -3.0564e+00,  3.7480e+00, -4.5357e-01],\n",
            "        [-3.0955e+00, -1.6958e+00,  4.0910e+00, -8.4772e-01, -2.1514e+00,\n",
            "          1.0599e+00, -9.9913e-01,  4.0498e-01,  2.4580e+00,  3.8107e+00,\n",
            "          6.0378e-03,  1.3877e+00, -8.1938e-01,  3.4637e+00,  2.0538e+00,\n",
            "          1.7851e-01, -4.9926e-01, -5.5215e-01, -2.1946e-01,  1.0937e+00,\n",
            "          6.6680e-01,  2.1294e+00, -2.9615e-01,  9.0184e-01, -4.2975e-01,\n",
            "          1.9310e-01,  1.0069e+00,  6.8021e-01, -2.5993e+00, -3.7769e-01,\n",
            "         -1.9033e+00, -5.1013e-01, -9.1239e-02,  1.5806e+00,  1.5196e+00,\n",
            "         -4.7918e+00, -8.3784e-02, -8.1818e-01, -5.8808e-01, -2.8268e+00,\n",
            "          3.0337e+00,  2.9031e+00,  1.1961e+00,  9.7041e-01, -2.3218e+00,\n",
            "         -1.4598e+00,  2.2256e+00, -3.1072e+00,  1.4538e+00,  2.1815e+00],\n",
            "        [-2.4711e+00, -2.5413e+00, -2.1583e-01, -3.4153e-01, -8.4134e-01,\n",
            "          7.8943e-01, -3.6214e+00,  7.2012e+00, -7.9381e-01,  2.4959e+00,\n",
            "          3.8375e+00,  3.3858e+00,  1.5536e+00, -6.5586e-01, -3.0257e+00,\n",
            "         -1.7034e+00,  1.9834e+00,  5.4113e+00, -1.6876e+00,  4.5347e+00,\n",
            "         -7.3100e+00,  5.0156e+00, -1.8919e+00, -2.9588e+00,  1.9567e+00,\n",
            "         -1.1386e+00, -1.9749e+00, -9.4741e-01, -1.1309e+00,  2.4528e+00,\n",
            "          4.3470e+00,  7.4043e+00, -1.7885e+00,  1.8292e+00, -3.9381e+00,\n",
            "          2.1951e+00, -1.0033e+00, -3.3128e+00,  1.0212e+00, -2.0837e+00,\n",
            "         -2.2628e+00,  1.7350e+00, -1.4000e+00,  6.2551e+00,  6.8410e-01,\n",
            "         -1.9006e+00,  2.5053e-01, -5.8300e+00, -2.1945e-01,  5.1248e-01],\n",
            "        [ 4.3944e+00, -2.2060e-01,  1.2579e+00, -1.5362e+00, -5.0562e+00,\n",
            "          5.1409e+00,  3.0545e+00,  2.2519e+00,  2.6118e+00,  1.7792e+00,\n",
            "         -1.0022e+00,  4.7934e+00, -1.5008e+00, -2.7649e-01,  7.2759e+00,\n",
            "          1.3521e+00,  3.2749e+00, -4.7944e-01,  6.6387e+00, -3.0759e+00,\n",
            "          5.4713e+00, -1.7941e+00,  1.5167e+00,  2.7838e+00, -3.9200e+00,\n",
            "          4.1551e+00, -7.2585e+00, -5.6637e+00, -3.8265e-01,  1.5666e+00,\n",
            "         -5.3162e+00,  3.1780e+00,  4.3107e+00,  4.7055e+00, -7.2620e+00,\n",
            "         -1.1213e+00, -2.2571e+00,  1.8715e+00, -1.8942e+00, -3.7499e+00,\n",
            "          5.5875e+00,  2.2895e+00,  7.8767e+00,  3.5828e-01, -3.8855e+00,\n",
            "         -1.3521e+00,  5.7630e-02, -9.2064e-02, -4.4719e-01, -1.8949e+00]]), tensor([[  1.8714, -12.7944,  20.7111,  -3.8728,  27.2779,  11.3714,   0.9856,\n",
            "         -25.4483,   1.6921, -12.8964,   1.4555, -10.0321,  10.9849,  -5.3070,\n",
            "         -19.4844,  -0.5668, -14.6160,  10.2280,   3.8711, -11.1332,  -8.3823,\n",
            "          14.7814,   4.6738,   6.3542,   3.3064,  16.3339,  -6.1230,  10.8460,\n",
            "         -12.7873,  17.4224,   3.2158,  -6.6880, -10.7006,  27.8619,  16.4612,\n",
            "          -1.9799,   8.4082,   2.7627,  -1.1836, -10.6745, -10.8995,   5.8157,\n",
            "          -8.9164,  -7.2573,   3.8021,  -1.2018,   7.2450, -16.5900,  -2.1766,\n",
            "         -28.4318],\n",
            "        [ -2.8761,  -0.1047,   3.1302,   1.2207,  -0.6261,  -0.8170,  -1.6928,\n",
            "           2.4066,  -0.1881,   2.3552,  -0.5737,   0.8195,   0.5662,   4.5821,\n",
            "           4.0540,  -3.1344,   2.7647,  -1.2270,  -2.2521,   0.1227,   0.7239,\n",
            "           1.6276,   0.7173,   0.2367,   0.6797,   0.9941,  -2.4215,  -1.0866,\n",
            "          -0.1012,   0.4476,   0.9802,  -0.9223,  -0.4524,   1.8197,  -0.9464,\n",
            "          -3.6617,  -1.3058,  -4.3949,   0.0968,  -1.0754,  -1.0050,   1.7984,\n",
            "           1.1912,  -2.3486,   0.2802,  -2.1400,   2.3239,  -4.1382,   2.7651,\n",
            "           5.7791],\n",
            "        [  0.6677,  -7.2961,   5.0347,   0.2190,  -8.2983,   0.7139,   2.2789,\n",
            "          -3.9322,  -1.4231,   3.4068,  -1.7365,   2.6461,   0.5069,  -1.4875,\n",
            "          -0.6076,  -4.0800,  -1.3011,  -3.9618,   3.4929,  -4.3759,  -3.3144,\n",
            "          -1.3587,  -3.2294,   3.6908,  -0.4545,   1.5266,   0.9728,  -0.9717,\n",
            "          -2.4866,  -1.3879,   2.3826,  -1.3146,   1.5436,   1.1973,  -6.1532,\n",
            "           3.9173,  -4.2900,  -3.9744,   4.2969,   3.9221,   1.4103,   2.1564,\n",
            "           0.8831,   3.6494,   2.2481,  -0.3199,   1.9781,   2.3958,  -0.7467,\n",
            "           5.1245],\n",
            "        [  0.5951,  -2.5271,   1.8674,   1.8833,   0.2563,  -8.5473,  -2.3622,\n",
            "           0.8526,   0.6200,   0.4974,  -4.5481,   3.1427,   2.9751,   4.2540,\n",
            "          11.1601,  -3.1624,   2.7492,  -1.0127,   0.2575,  -6.1984,  -1.0338,\n",
            "           5.9638,   5.4553,  -0.6256,   6.0304,   6.5881,  -2.2765,  -5.6206,\n",
            "          -2.0587,   0.7040,   2.6823,   1.8052,  -1.4786,   7.6757,  -3.6304,\n",
            "          -3.1240,  -3.1849,   2.3458,  -4.6595,  -9.4814,   0.4839,  -1.1548,\n",
            "           6.5542,   0.0932,   0.9327,  -1.4085,  -5.2103,  -0.9270,  -0.4646,\n",
            "           8.4012],\n",
            "        [  3.1115, -11.3418,  -3.5410,  -4.7156,   1.0952,   8.6683,   3.3720,\n",
            "          -3.0535,  -5.6487,   9.4131,  -2.1633,   1.5622,  -0.7084,   2.7956,\n",
            "          -8.4753,   8.0211,  -2.9571,   5.7622,  -1.3355,  -5.3748,  11.9042,\n",
            "           7.9770,  -4.5245,  -1.2853,  -6.2614,   5.8585,  -5.1954,   2.4161,\n",
            "          -2.3827,   0.5276,   6.5253,   0.8573,  -5.3990,   1.9370,   0.4902,\n",
            "          -4.0552,   0.6371,   5.0046,   2.0239,  -1.1665,   1.6528,   5.3939,\n",
            "           4.6760,   3.6058,  -0.9550,  -2.5279,   7.3697,  -3.7577,  -0.1596,\n",
            "          -3.9396],\n",
            "        [  4.9214,  -4.3273,  -4.5023,  -0.7450,   9.0083,   3.7828,   0.4438,\n",
            "           4.4870,  -5.3341,  -0.0417,  -4.0001,   7.5403,   4.6823,  -5.4328,\n",
            "          -1.3512,  -1.1382,   0.1027,  -6.4553,   0.2548,  -2.0287,  -0.7580,\n",
            "           1.5775,   0.9875,   5.3831,  -4.1517,   4.6206,   0.1222,  -5.7749,\n",
            "          -9.0991,   8.0723,  -9.4251,  -1.2145,   0.6025,   4.8888,  -3.0626,\n",
            "          -2.5659,  -2.0906,   2.9844,  -0.5761,  -2.0550,  -0.2343,   1.1179,\n",
            "           8.6681,   6.2497,  -0.2923,  -0.9859,  -1.0562,  -6.1917,   1.5471,\n",
            "          -0.1579],\n",
            "        [  0.1483,   1.8934,   0.3150,   1.5030,  -2.0540,  -1.5031,  -1.6990,\n",
            "           1.1564,  -6.3275,   0.7063,  -1.6843,  -2.4318,   0.8433,  -2.5089,\n",
            "           5.6061,  -6.8078,   2.5904,   0.7564,  -2.2916,  -3.8474,  -4.5019,\n",
            "          -1.9314,  -2.6870,  -4.4165,  -0.3054,   4.8795,   3.1082,  -4.1619,\n",
            "          -1.3200,  -1.8925,  -0.3165,  -3.7242,  -3.1784,  -1.4785,  -3.3280,\n",
            "           4.0205,  -2.9676,  -6.8249,  -2.9460,   1.1627,  -2.2618,   3.9812,\n",
            "          -1.5496,   6.2229,  -1.9712,  -5.6869,   2.7972,  -0.9743,   1.4847,\n",
            "           3.4626]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm.fit(torch.cat(features).numpy(), Y_train)\n",
        "\n",
        "X_test_data = SvmDataset(X_test_scaled)\n",
        "dataloader2 = DataLoader(X_test_data, batch_size=10, shuffle=False)\n",
        "\n",
        "autoencoder.eval()\n",
        "features2 = []\n",
        "actual2 = []\n",
        "for batch in dataloader2:\n",
        "    if (isinstance(batch, tuple) or isinstance(batch, list)) and len(batch) == 2:\n",
        "        batch, value = batch  # if we have a prediction label, separate it to actual\n",
        "        actual2.append(value)\n",
        "    batch = batch.cuda(non_blocking=True)\n",
        "    features2.append(autoencoder.encoder(batch).detach().cpu())\n",
        "  \n",
        "predicted = svm.predict(torch.cat(features2))\n",
        "\n",
        "print(predicted)\n",
        "print(\"########################\")\n",
        "print(Y_test)\n",
        "error = np.mean(predicted != Y_test)\n",
        "print(\"Error of the computed SVM:\", error)\n",
        "#################################################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUjt8tlW7eC_",
        "outputId": "9df3a75a-3f77-4e7c-e738-6973b590c9c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ballantyne, R. M. (Robert Michael)' 'Burnett, Frances Hodgson'\n",
            " 'Spinoza, Benedictus de' 'Twain, Mark' 'Locke, William John'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Arthur, T. S. (Timothy Shay)'\n",
            " 'Duncan, Norman' 'Ballantyne, R. M. (Robert Michael)' 'Ebers, Georg'\n",
            " 'Lytton, Edward Bulwer Lytton, Baron' 'Dewey, John'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Molière' 'Dewey, John'\n",
            " 'Churchill, Winston' 'Burnett, Frances Hodgson' 'Molesworth, Mrs.'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Burroughs, John'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Molesworth, Mrs.'\n",
            " 'Shakespeare, William' 'Churchill, Winston' 'Shakespeare, William'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Jacobs, W. W. (William Wymark)'\n",
            " 'Arthur, T. S. (Timothy Shay)' 'Dewey, John'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Ballantyne, R. M. (Robert Michael)'\n",
            " 'Burnett, Frances Hodgson' 'London, Jack' 'Dewey, John' 'Dewey, John'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Molesworth, Mrs.' 'Twain, Mark'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Locke, William John'\n",
            " 'Churchill, Winston' 'Locke, William John' 'Churchill, Winston'\n",
            " 'Ballantyne, R. M. (Robert Michael)' 'Rinehart, Mary Roberts' 'Molière'\n",
            " 'Mulford, Clarence Edward']\n",
            "########################\n",
            "['Burnett, Frances Hodgson', 'London, Jack', 'Spinoza, Benedictus de', 'Shakespeare, William', 'Warner, Charles Dudley', 'Paine, Albert Bigelow', 'Howells, William Dean', 'Arthur, T. S. (Timothy Shay)', 'Churchill, Winston', 'Ebers, Georg', 'Twain, Mark', 'Holmes, Oliver Wendell', 'Lytton, Edward Bulwer Lytton, Baron', 'Molière', 'Whittier, John Greenleaf', 'Cicero, Marcus Tullius', 'Glyn, Elinor', 'Quiller-Couch, Arthur', 'Rinehart, Mary Roberts', 'Jacobs, W. W. (William Wymark)', 'Bennett, Arnold', 'Molesworth, Mrs.', 'Shoghi, Effendi', 'Ballantyne, R. M. (Robert Michael)', 'Rohmer, Sax', 'Lang, Andrew', 'Smith, Francis Hopkinson', 'Lowndes, Marie Belloc', 'Duncan, Norman', 'Locke, William John', 'Holt, Emily Sarah', 'Pemberton, Max', 'Burroughs, John', 'Reynolds, Mack', 'Wallace, F. L. (Floyd L.)', 'Barr, Amelia E.', 'Raymond, Evelyn', 'Hall, E. Raymond (Eugene Raymond)', 'Bangs, John Kendrick', 'Dewey, John', 'Roy, Lillian Elizabeth', 'Drake, Samuel Adams', 'Leinster, Murray', 'Alger, Horatio, Jr.', 'Phillpotts, Eden', 'Hawkins, N. (Nehemiah)', 'Mulford, Clarence Edward']\n",
            "Error of the computed SVM: 0.8936170212765957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-432c1cd8f16b>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self._cache[index] = ((torch.tensor(self.dataset[index])), 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Testing out the code from the example for ptsdae**\n",
        "==\n",
        "\n"
      ],
      "metadata": {
        "id": "ugabQ_mZeC9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vlukiyanov/pt-sdae"
      ],
      "metadata": {
        "id": "ofYeTPQHeYt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pt-sdae"
      ],
      "metadata": {
        "id": "0f3uSqYieZTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "id": "iYbUvyCMef15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall cytoolz"
      ],
      "metadata": {
        "id": "bH_wz_d-eklx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cytoolz"
      ],
      "metadata": {
        "id": "iOFhe2VOepk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co6waEDFC5Xm",
        "outputId": "4c7d1a74-d315-4701-ef4d-9fc857e749f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (3.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (23.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import click\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from tensorboardX import SummaryWriter\n",
        "from sklearn.cluster import KMeans\n",
        "import uuid\n",
        "\n",
        "import tensorflow as tf \n",
        "\n",
        "from ptsdae.sdae import StackedDenoisingAutoEncoder\n",
        "import ptsdae.model as ae\n",
        "from ptsdae.utils import cluster_accuracy\n"
      ],
      "metadata": {
        "id": "kGhn2Cz1Bp1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CachedMNIST(Dataset):\n",
        "    def __init__(self, train, cuda, testing_mode=False):\n",
        "        img_transform = transforms.Compose([transforms.Lambda(self._transformation)])\n",
        "        self.ds = MNIST(\"./data\", download=True, train=train, transform=img_transform)\n",
        "        self.cuda = cuda\n",
        "        self.testing_mode = testing_mode\n",
        "        self._cache = dict()\n",
        "\n",
        "    @staticmethod\n",
        "    def _transformation(img):\n",
        "        return (\n",
        "            torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).float()\n",
        "            * 0.02\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index: int) -> torch.Tensor:\n",
        "        if index not in self._cache:\n",
        "            self._cache[index] = list(self.ds[index])\n",
        "            if self.cuda:\n",
        "                self._cache[index][0] = self._cache[index][0].cuda(non_blocking=True)\n",
        "                self._cache[index][1] = torch.tensor(self._cache[index][1]).cuda(\n",
        "                    non_blocking=True\n",
        "                )\n",
        "        return self._cache[index]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return 128 if self.testing_mode else len(self.ds)\n"
      ],
      "metadata": {
        "id": "d0N8Pva3DE8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(cuda=True, batch_size=128, pretrain_epochs=5, finetune_epochs=10, testing_mode=False):\n",
        "    writer = SummaryWriter()  # create the TensorBoard object\n",
        "    # callback function to call during training, uses writer from the scope\n",
        "\n",
        "    def training_callback(epoch, lr, loss, validation_loss):\n",
        "        writer.add_scalars(\n",
        "            \"data/autoencoder\",\n",
        "            {\"lr\": lr, \"loss\": loss, \"validation_loss\": validation_loss,},\n",
        "            epoch,\n",
        "        )\n",
        "\n",
        "    ds_train = CachedMNIST(\n",
        "        train=True, cuda=cuda, testing_mode=testing_mode\n",
        "    )  # training dataset\n",
        "    ds_val = CachedMNIST(\n",
        "        train=False, cuda=cuda, testing_mode=testing_mode\n",
        "    )  # evaluation dataset\n",
        "    autoencoder = StackedDenoisingAutoEncoder(\n",
        "        [28 * 28, 500, 500, 2000, 10], final_activation=None\n",
        "    )\n",
        "    if cuda:\n",
        "        autoencoder.cuda()\n",
        "    print(\"Pretraining stage.\")\n",
        "    ae.pretrain(\n",
        "        ds_train,\n",
        "        autoencoder,\n",
        "        cuda=cuda,\n",
        "        validation=ds_val,\n",
        "        epochs=pretrain_epochs,\n",
        "        batch_size=batch_size,\n",
        "        optimizer=lambda model: SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
        "        scheduler=lambda x: StepLR(x, 100, gamma=0.1),\n",
        "        corruption=0.2,\n",
        "    )\n",
        "    print(\"Training stage.\")\n",
        "    ae_optimizer = SGD(params=autoencoder.parameters(), lr=0.1, momentum=0.9)\n",
        "    ae.train(\n",
        "        ds_train,\n",
        "        autoencoder,\n",
        "        cuda=cuda,\n",
        "        validation=ds_val,\n",
        "        epochs=finetune_epochs,\n",
        "        batch_size=batch_size,\n",
        "        optimizer=ae_optimizer,\n",
        "        scheduler=StepLR(ae_optimizer, 100, gamma=0.1),\n",
        "        corruption=0.2,\n",
        "        update_callback=training_callback,\n",
        "    )\n",
        "    print(\"k-Means stage\")\n",
        "    dataloader = DataLoader(ds_train, batch_size=50, shuffle=False)\n",
        "    kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "    autoencoder.eval()\n",
        "    features = []\n",
        "    actual = []\n",
        "    for batch in dataloader:\n",
        "        if (isinstance(batch, tuple) or isinstance(batch, list)) and len(batch) == 2:\n",
        "            batch, value = batch  # if we have a prediction label, separate it to actual\n",
        "            actual.append(value)\n",
        "        if cuda:\n",
        "            batch = batch.cuda(non_blocking=True)\n",
        "        features.append(autoencoder.encoder(batch).detach().cpu())\n",
        "    actual = torch.cat(actual).long().cpu().numpy()\n",
        "    predicted = kmeans.fit_predict(torch.cat(features).numpy())\n",
        "    reassignment, accuracy = cluster_accuracy(actual, predicted)\n",
        "    print(\"Final k-Means accuracy: %s\" % accuracy)\n",
        "    predicted_reassigned = [reassignment[item] for item in predicted]  # TODO numpify\n",
        "    if not testing_mode:\n",
        "        confusion = confusion_matrix(actual, predicted_reassigned)\n",
        "        normalised_confusion = (\n",
        "            confusion.astype(\"float\") / confusion.sum(axis=1)[:, np.newaxis]\n",
        "        )\n",
        "        confusion_id = uuid.uuid4().hex\n",
        "        sns.heatmap(normalised_confusion).get_figure().savefig(\n",
        "            \"confusion_%s.png\" % confusion_id\n",
        "        )\n",
        "        print(\"Writing out confusion diagram with UUID: %s\" % confusion_id)\n",
        "        writer.add_embedding(\n",
        "            torch.cat(features),\n",
        "            metadata=predicted,\n",
        "            label_img=ds_train.ds.train_data.float().unsqueeze(1),  # TODO bit ugly\n",
        "            tag=\"predicted\",\n",
        "        )\n",
        "        writer.close()"
      ],
      "metadata": {
        "id": "L5YNtAidDIO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28bnPZiXDYv1",
        "outputId": "c13367f4-dde5-4f17-defc-6ed23e9d63d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 320175207.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 24986735.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 159070115.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8232726.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretraining stage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?batch/s, epo=0, lss=0.000000, vls=-1.000000]<ipython-input-20-b1a5b93bd95a>:12: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).float()\n",
            "100%|██████████| 469/469 [00:10<00:00, 45.00batch/s, epo=0, lss=0.912399, vls=-1.000000]\n",
            "100%|██████████| 469/469 [00:01<00:00, 337.51batch/s, epo=1, lss=0.838322, vls=0.661521]\n",
            "100%|██████████| 469/469 [00:01<00:00, 333.54batch/s, epo=2, lss=0.845350, vls=0.617398]\n",
            "100%|██████████| 469/469 [00:01<00:00, 323.89batch/s, epo=3, lss=0.840369, vls=0.589264]\n",
            "100%|██████████| 469/469 [00:01<00:00, 338.55batch/s, epo=4, lss=0.807580, vls=0.588086]\n",
            "100%|██████████| 469/469 [00:01<00:00, 304.72batch/s, epo=0, lss=0.138238, vls=-1.000000]\n",
            "100%|██████████| 469/469 [00:01<00:00, 301.27batch/s, epo=1, lss=0.106051, vls=0.070806]\n",
            "100%|██████████| 469/469 [00:01<00:00, 312.35batch/s, epo=2, lss=0.093441, vls=0.036715]\n",
            "100%|██████████| 469/469 [00:01<00:00, 311.06batch/s, epo=3, lss=0.088073, vls=0.029516]\n",
            "100%|██████████| 469/469 [00:01<00:00, 304.17batch/s, epo=4, lss=0.075029, vls=0.025012]\n",
            "100%|██████████| 469/469 [00:01<00:00, 306.99batch/s, epo=0, lss=0.027141, vls=-1.000000]\n",
            "100%|██████████| 469/469 [00:01<00:00, 302.73batch/s, epo=1, lss=0.019749, vls=0.018212]\n",
            "100%|██████████| 469/469 [00:01<00:00, 304.45batch/s, epo=2, lss=0.015697, vls=0.011066]\n",
            "100%|██████████| 469/469 [00:01<00:00, 311.47batch/s, epo=3, lss=0.014023, vls=0.008563]\n",
            "100%|██████████| 469/469 [00:01<00:00, 289.07batch/s, epo=4, lss=0.012914, vls=0.007253]\n",
            "100%|██████████| 469/469 [00:01<00:00, 299.74batch/s, epo=0, lss=0.007016, vls=-1.000000]\n",
            "100%|██████████| 469/469 [00:01<00:00, 313.77batch/s, epo=1, lss=0.005468, vls=0.006848]\n",
            "100%|██████████| 469/469 [00:01<00:00, 312.00batch/s, epo=2, lss=0.006008, vls=0.006104]\n",
            "100%|██████████| 469/469 [00:01<00:00, 312.53batch/s, epo=3, lss=0.005727, vls=0.005811]\n",
            "100%|██████████| 469/469 [00:01<00:00, 310.25batch/s, epo=4, lss=0.005582, vls=0.005693]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training stage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:01<00:00, 244.39batch/s, epo=0, lss=0.688162, vls=-1.000000]\n",
            "100%|██████████| 469/469 [00:01<00:00, 237.08batch/s, epo=1, lss=0.597030, vls=0.631359]\n",
            "100%|██████████| 469/469 [00:01<00:00, 242.46batch/s, epo=2, lss=0.654951, vls=0.596604]\n",
            "100%|██████████| 469/469 [00:01<00:00, 242.05batch/s, epo=3, lss=0.560482, vls=0.569976]\n",
            "100%|██████████| 469/469 [00:01<00:00, 242.70batch/s, epo=4, lss=0.506502, vls=0.548301]\n",
            "100%|██████████| 469/469 [00:01<00:00, 241.01batch/s, epo=5, lss=0.571916, vls=0.526016]\n",
            "100%|██████████| 469/469 [00:01<00:00, 238.14batch/s, epo=6, lss=0.528559, vls=0.517470]\n",
            "100%|██████████| 469/469 [00:01<00:00, 235.69batch/s, epo=7, lss=0.526773, vls=0.501225]\n",
            "100%|██████████| 469/469 [00:01<00:00, 242.82batch/s, epo=8, lss=0.504555, vls=0.482090]\n",
            "100%|██████████| 469/469 [00:01<00:00, 239.89batch/s, epo=9, lss=0.478914, vls=0.480499]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k-Means stage\n",
            "Final k-Means accuracy: 0.7319333333333333\n",
            "Writing out confusion diagram with UUID: b031071b4fbf4a2087383d08af0be23b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(cuda=True, batch_size=128, pretrain_epochs=30, finetune_epochs=10, testing_mode=False):\n",
        "    writer = SummaryWriter()  # create the TensorBoard object\n",
        "    # callback function to call during training, uses writer from the scope\n",
        "\n",
        "    def training_callback(epoch, lr, loss, validation_loss):\n",
        "        writer.add_scalars(\n",
        "            \"data/autoencoder\",\n",
        "            {\"lr\": lr, \"loss\": loss, \"validation_loss\": validation_loss,},\n",
        "            epoch,\n",
        "        )\n",
        "\n",
        "    ds_train = CachedMNIST(\n",
        "        train=True, cuda=cuda, testing_mode=testing_mode\n",
        "    )  # training dataset\n",
        "    ds_val = CachedMNIST(\n",
        "        train=False, cuda=cuda, testing_mode=testing_mode\n",
        "    )  # evaluation dataset\n",
        "    autoencoder = StackedDenoisingAutoEncoder(\n",
        "        [28 * 28, 500, 500, 2000, 10], final_activation=None\n",
        "    )\n",
        "    print(ds_train.__getitem__(2))\n",
        "    print((ds_train.__getitem__(2)[0]).shape)\n",
        "\n",
        "    dataloader = DataLoader(ds_train, batch_size=1024, shuffle=False)\n",
        "\n",
        "    print(len(dataloader))\n",
        "  "
      ],
      "metadata": {
        "id": "rI0ptxBXHR1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrPKDV5UH1Ek",
        "outputId": "2edc853c-a7d9-4296-8d97-e197aa078436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3400, 4.6400,\n",
            "        0.7800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 1.2400, 1.6200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4000,\n",
            "        3.6000, 0.7800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 2.5200, 3.2600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400,\n",
            "        3.0600, 4.2000, 0.8000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 4.4000, 3.2600, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.5400, 5.0800, 3.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 4.4400, 3.2600, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 3.6600, 5.0800, 2.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.9200, 4.9000, 3.2600, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 3.9600, 5.0800, 1.1200, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4000, 5.0800, 3.2600, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.4600, 4.6200, 5.0800, 0.5800, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1800, 5.0800, 2.4000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 3.2600, 5.0800, 4.3200, 0.3200, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1800, 5.0800,\n",
            "        1.3400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.2800, 1.7200, 3.5600, 4.9600, 5.0800, 1.8200, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1800,\n",
            "        5.0800, 1.7000, 0.0000, 0.0000, 0.0000, 0.9400, 0.9800, 2.3200, 2.8800,\n",
            "        3.0000, 4.8200, 4.8600, 4.6800, 3.5800, 4.8200, 5.0400, 0.8000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        3.0000, 5.0600, 4.7400, 4.1400, 4.1400, 4.1400, 5.0600, 5.0800, 5.0000,\n",
            "        4.8000, 3.9600, 2.8600, 1.8200, 0.5600, 0.1000, 4.6600, 5.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 2.3800, 3.5400, 3.5400, 3.5400, 3.5400, 3.5400, 1.9600,\n",
            "        1.1200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0400, 5.0800, 4.4000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3800, 5.0800,\n",
            "        2.7400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3800,\n",
            "        5.0800, 1.1400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        3.3800, 5.0800, 1.1400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 3.3800, 5.1000, 1.8800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 3.3800, 5.0800, 1.9200, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 3.3800, 5.0800, 3.0600, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 3.3800, 5.1000, 3.0600, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.9200, 5.0800, 3.0600, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000]), 4]\n",
            "60000\n",
            "hihi\n",
            "[tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.7600, 5.0800, 2.1800, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 1.7400, 5.0400, 1.6400, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 2.7000, 4.8200, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 4.8800, 3.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6800, 5.0800, 1.2600, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.0400, 4.4600, 0.2200,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6400, 5.0800, 4.3200,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.9000, 5.0800,\n",
            "        3.9000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8000,\n",
            "        5.0800, 1.5400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1400,\n",
            "        4.7400, 4.1000, 0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        2.4800, 5.1000, 3.3000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 3.4200, 5.0800, 1.6200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.4800, 4.6400, 4.3000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 2.4000, 5.0800, 3.1800, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 3.0200, 5.0800, 2.8400, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 4.5600, 5.0800, 1.3200, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 1.2200, 5.0200, 5.0800, 1.3200, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8200, 5.0800, 4.1000, 0.0600,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 4.3000, 5.0800, 2.4200,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 3.9600, 3.5200,\n",
            "        0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000]), 1]\n",
            "10000\n",
            "hihi\n",
            "[tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3400, 4.6400,\n",
            "        0.7800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 1.2400, 1.6200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4000,\n",
            "        3.6000, 0.7800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 2.5200, 3.2600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400,\n",
            "        3.0600, 4.2000, 0.8000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 4.4000, 3.2600, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.5400, 5.0800, 3.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 4.4400, 3.2600, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 3.6600, 5.0800, 2.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.9200, 4.9000, 3.2600, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 3.9600, 5.0800, 1.1200, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4000, 5.0800, 3.2600, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.4600, 4.6200, 5.0800, 0.5800, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1800, 5.0800, 2.4000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 3.2600, 5.0800, 4.3200, 0.3200, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1800, 5.0800,\n",
            "        1.3400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.2800, 1.7200, 3.5600, 4.9600, 5.0800, 1.8200, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1800,\n",
            "        5.0800, 1.7000, 0.0000, 0.0000, 0.0000, 0.9400, 0.9800, 2.3200, 2.8800,\n",
            "        3.0000, 4.8200, 4.8600, 4.6800, 3.5800, 4.8200, 5.0400, 0.8000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        3.0000, 5.0600, 4.7400, 4.1400, 4.1400, 4.1400, 5.0600, 5.0800, 5.0000,\n",
            "        4.8000, 3.9600, 2.8600, 1.8200, 0.5600, 0.1000, 4.6600, 5.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 2.3800, 3.5400, 3.5400, 3.5400, 3.5400, 3.5400, 1.9600,\n",
            "        1.1200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0400, 5.0800, 4.4000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3800, 5.0800,\n",
            "        2.7400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3800,\n",
            "        5.0800, 1.1400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        3.3800, 5.0800, 1.1400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 3.3800, 5.1000, 1.8800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 3.3800, 5.0800, 1.9200, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 3.3800, 5.0800, 3.0600, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 3.3800, 5.1000, 3.0600, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.9200, 5.0800, 3.0600, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000], device='cuda:0'), tensor(4, device='cuda:0')]\n",
            "torch.Size([784])\n",
            "59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-b86a47cfb085>:15: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).float()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using DAE from keras library**\n",
        "=="
      ],
      "metadata": {
        "id": "4ufYe3avdgbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "# Classifier\n",
        "from sklearn.svm import SVC\n",
        "# Character N-gram feature extractor\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# SDAE and model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "# Util\n",
        "from data_io import get_book\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "-dVezYyJVR8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "val_data = pd.read_csv(\"val.csv\")\n",
        "\n",
        "cv = CountVectorizer(analyzer='char', ngram_range=(2, 4), stop_words='english')\n",
        "X_train, X_test, X_val = cv.fit_transform(train_data.text.tolist()), cv.transform(test_data.text.tolist()), cv.transform(val_data.text.tolist())  \n",
        "Y_train, Y_test, Y_val = train_data.author.tolist(), test_data.author.tolist(), val_data.author.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pxn8irWVaZX",
        "outputId": "88b79b8b-cef7-4cca-b51e-4ae7a0cee20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_test, X_val = X_train.toarray(), X_test.toarray(), X_val.toarray()\n",
        "\n",
        "scaler_train = StandardScaler()\n",
        "scaler_train.fit(X_train)\n",
        "X_train_scaled = scaler_train.transform(X_train)\n",
        "\n",
        "scaler_test = StandardScaler()\n",
        "scaler_test.fit(X_test)\n",
        "X_test_scaled = scaler_test.transform(X_test)\n",
        "\n",
        "scaler_val = StandardScaler()\n",
        "scaler_val.fit(X_val)\n",
        "X_val_scaled = scaler_val.transform(X_val)\n",
        "\n",
        "# Add Gaussian noise\n",
        "noise_factor = 0.1\n",
        "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1., size=X_train.shape)\n",
        "scaler_noisy = StandardScaler()\n",
        "scaler_noisy.fit(X_train_noisy)\n",
        "X_train_noisy_scaled1 = scaler_noisy.transform(X_train_noisy)\n",
        "\n",
        "X_train_noisy_scaled = X_train_scaled + noise_factor * np.random.normal(loc=0.0, scale=1., size=X_train.shape)\n",
        "\n",
        "scaler_noisy2 = StandardScaler()\n",
        "scaler_noisy2.fit(X_train_noisy_scaled)\n",
        "X_train_noisy_scaled2 = scaler_noisy2.transform(X_train_noisy_scaled)\n",
        "\n",
        "print(X_train_scaled)\n",
        "print(X_train_noisy_scaled1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQHmTN_TZ6gw",
        "outputId": "92ced40c-1763-490e-89bb-b7be407e79e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.399956    1.22775537  0.         ... -0.14744196 -0.14744196\n",
            "  -0.14744196]\n",
            " [-0.51226334 -0.68690124 -0.59347976 ... -0.14744196 -0.14744196\n",
            "  -0.14744196]\n",
            " [ 0.60230521  0.85951239  0.11869595 ... -0.14744196 -0.14744196\n",
            "  -0.14744196]\n",
            " ...\n",
            " [ 0.40880683  0.35059232  3.67957451 ...  6.78232998  6.78232998\n",
            "   6.78232998]\n",
            " [ 0.00841917  0.28335694 -0.2373919  ... -0.14744196 -0.14744196\n",
            "  -0.14744196]\n",
            " [ 0.0358705   0.10751058 -0.11869595 ... -0.14744196 -0.14744196\n",
            "  -0.14744196]]\n",
            "[[ 1.39997994e+00  1.22782632e+00  7.72688486e-04 ...  1.41946904e-02\n",
            "  -7.95291223e-01  8.24077889e-02]\n",
            " [-5.12296164e-01 -6.86927109e-01 -5.91806809e-01 ... -7.93763507e-03\n",
            "   8.86765082e-01 -2.89515330e-01]\n",
            " [ 6.02322805e-01  8.59479451e-01  1.21523012e-01 ...  8.79759881e-02\n",
            "  -7.84882166e-01 -3.75581629e-01]\n",
            " ...\n",
            " [ 4.08820793e-01  3.50567157e-01  3.68903142e+00 ...  6.60545974e+00\n",
            "   5.56212227e+00  6.10458043e+00]\n",
            " [ 8.41577482e-03  2.83440554e-01 -2.50421730e-01 ... -3.26589072e-02\n",
            "  -6.61004317e-01 -4.14729097e-01]\n",
            " [ 3.58496347e-02  1.07584892e-01 -1.33975497e-01 ... -6.87616654e-01\n",
            "   5.37722568e-01 -2.39512564e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (X_train_scaled.shape[1],)\n",
        "encoder_input = Input(shape=input_shape)"
      ],
      "metadata": {
        "id": "8edeTeaLYGbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Dense(50, activation='relu')(encoder_input)\n",
        "decoder = Dense(X_train.shape[1], activation='sigmoid')(encoder)"
      ],
      "metadata": {
        "id": "EHDIseDIVgU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other implementations of DAE\n",
        "encoder = Dense(49, activation='relu')(encoder_input)\n",
        "encoder = Dense(20, activation='relu')(encoder)\n",
        "encoder = Dense(10, activation='relu')(encoder)\n",
        "\n",
        "decoder = Dense(20, activation='relu')(encoder)\n",
        "decoder = Dense(40, activation='relu')(decoder)\n",
        "decoder = Dense(X_train.shape[1], activation='sigmoid')(decoder)"
      ],
      "metadata": {
        "id": "zpsCsisOsqOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Model(encoder_input, decoder)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "oUKDJcgzVkxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(X_train_noisy_scaled2, X_train_scaled, epochs=20, batch_size=10, validation_data=(X_val_scaled, X_val_scaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qW2ecj8Vow9",
        "outputId": "7d33dcaa-91ab-466f-ba90-18d4cf9974ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 0s 56ms/step - loss: -15394.3008 - val_loss: -16839.5703\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 30ms/step - loss: -21451.6348 - val_loss: -23393.5137\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 39ms/step - loss: -30023.0742 - val_loss: -32009.7656\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 32ms/step - loss: -39455.2656 - val_loss: -43435.5742\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 32ms/step - loss: -54012.2656 - val_loss: -58206.6523\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 30ms/step - loss: -72488.5469 - val_loss: -77263.6719\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 33ms/step - loss: -97431.8203 - val_loss: -101579.2109\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 30ms/step - loss: -124017.5078 - val_loss: -131284.4375\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 31ms/step - loss: -162006.8125 - val_loss: -168422.3438\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 55ms/step - loss: -209354.8906 - val_loss: -215527.5156\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 41ms/step - loss: -264270.7500 - val_loss: -275681.8438\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 56ms/step - loss: -342562.7812 - val_loss: -348235.2188\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 42ms/step - loss: -423425.8750 - val_loss: -437871.7500\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 41ms/step - loss: -539394.8125 - val_loss: -542424.0625\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 34ms/step - loss: -671651.7500 - val_loss: -671472.0625\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 35ms/step - loss: -825663.5000 - val_loss: -831172.0625\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 48ms/step - loss: -1028714.4375 - val_loss: -1018000.9375\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 42ms/step - loss: -1248738.1250 - val_loss: -1249042.6250\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 41ms/step - loss: -1542031.0000 - val_loss: -1523423.5000\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 40ms/step - loss: -1874772.6250 - val_loss: -1852911.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11bf185ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Model(encoder_input, encoder)\n",
        "X_train_encoded = encoder.predict(X_train_scaled)\n",
        "X_test_encoded = encoder.predict(X_test_scaled)\n",
        "X_val_encoded = encoder.predict(X_val_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "1Jn7PJ_8VqMz",
        "outputId": "087d4a70-950d-4c4f-ee9d-466b67bb2c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-34fb7d2ebdc0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_val_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 )\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_keras_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_graph_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# A Network does not create weights of its own, thus it is already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_validate_graph_inputs_and_outputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_history\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m                 \u001b[0mcls_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    873\u001b[0m                     \u001b[0;34mf\"Output tensors of a {cls_name} model must be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                     \u001b[0;34m\"the output of a TensorFlow `Layer` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Output tensors of a Functional model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: <keras.engine.functional.Functional object at 0x7f11be675880>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an SVM classifier on the encoded feature vectors\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train_encoded, Y_train)\n",
        "\n",
        "# Evaluate the trained SVM on the test and validation sets\n",
        "print(\"Accuracy on train set:\", svm.score(X_train_encoded, Y_train))\n",
        "print(\"Accuracy on test set:\", svm.score(X_test_encoded, Y_test))\n",
        "print(\"Accuracy on validation set:\", svm.score(X_val_encoded, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRWFY9hmVwjD",
        "outputId": "8c649fa5-0117-48d1-ee78-7394b27d15a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train set: 1.0\n",
            "Accuracy on test set: 0.0\n",
            "Accuracy on validation set: 0.02127659574468085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other implementations of DAE**"
      ],
      "metadata": {
        "id": "0qhjFY-6gjDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SDAEModel:\n",
        "  def __init__():\n",
        "    pass\n",
        "  \n",
        "  def forward(input_):\n",
        "    encoder_input = Input(shape = (input_.shape[1], ))\n",
        "    encoder = Dense(500, activation='relu')(encoder_input)\n",
        "    encoder = Dense(250, activation='relu')(encoder)\n",
        "    encoder = Dense(100, activation='relu')(encoder)\n",
        "\n",
        "    decoder = Dense(250, activation='relu')(encoder)\n",
        "    decoder = Dense(500, activation='relu')(decoder)\n",
        "    decoder = Dense(X_train.shape[1], activation='sigmoid')(decoder)\n",
        "\n",
        "    return "
      ],
      "metadata": {
        "id": "e18CAUxhghio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Using SDAE from SDAE library**\n",
        "==\n",
        "\n"
      ],
      "metadata": {
        "id": "NI2fXvwxfQII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MadhumitaSushil/SDAE"
      ],
      "metadata": {
        "id": "L8SCvjBmfQuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd SDAE"
      ],
      "metadata": {
        "id": "7Zq5RKvnfXmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Classifier\n",
        "from sklearn.svm import SVC\n",
        "# Character N-gram feature extractor\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# SDAE and model\n",
        "from ptsdae.sdae import StackedDenoisingAutoEncoder\n",
        "from ptsdae import model as ae\n",
        "#from sdae import StackedDenoisingAE\n",
        "# Util\n",
        "from data_io import get_book\n",
        "import torch"
      ],
      "metadata": {
        "id": "1WLdTN80ffCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "val_data = pd.read_csv(\"val.csv\")\n",
        "\n",
        "cv = CountVectorizer(analyzer='char', ngram_range=(1, 3), stop_words='english', dtype=np.float32)\n",
        "X_train, X_test, X_val = cv.fit_transform(train_data.text.tolist()), cv.transform(test_data.text.tolist()), cv.transform(val_data.text.tolist())  \n",
        "Y_train, Y_test, Y_val = train_data.author.tolist(), test_data.author.tolist(), val_data.author.tolist()"
      ],
      "metadata": {
        "id": "i_cCZVwLfiS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdae = StackedDenoisingAE(n_layers = 3, \n",
        "                          n_hid = [X_train.shape[1], \n",
        "                          256, 128], dropout = [0.1],\n",
        "                          enc_act = ['relu'], dec_act = ['relu', 'relu', 'sigmoid'],\n",
        "                          nb_epoch = 30, optimizer = 'adam', batch_size = 27)"
      ],
      "metadata": {
        "id": "l_pMu2fEfi_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, (dense_train, dense_val, dense_test), recon_mse = sdae.get_pretrained_sda(X_train,\n",
        "                                                                                 X_val,\n",
        "                                                                                 X_test,\n",
        "                                                                                 dir_out = '../output/')"
      ],
      "metadata": {
        "id": "u1ChKPLifuhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}